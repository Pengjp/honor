{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rEP8Ti9V-1B4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, layers, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU,Conv1D,Dropout,MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten,ReLU, BatchNormalization,GlobalAveragePooling1D\n",
    "import numpy as np\n",
    "\n",
    "def runif():\n",
    "    return tf.random.uniform([1], dtype=tf.float64)[0]\n",
    "    # return tf.constant(.8, tf.float32)\n",
    "\n",
    "def rexp():\n",
    "    return -tf.math.log(runif())\n",
    "\n",
    "\n",
    "def exprelu(x):\n",
    "    return tf.where(x > 0, tf.math.expm1(x), tf.zeros_like(x))\n",
    "\n",
    "def reloid(x):\n",
    "    \"(sigma(x[1]), ..., sigma(x[-2]), relu(x[-1])\"\n",
    "    return tf.concat([tf.nn.sigmoid(x[:-1]), tf.math.exp(x[-1:])], axis=0)\n",
    "\n",
    "def reloid_derivative(x):\n",
    "    return tf.concat(\n",
    "        [\n",
    "            tf.nn.sigmoid(x[:-1])\n",
    "            * (1 - tf.nn.sigmoid(x[:-1])),  # derivative of sigmoid\n",
    "            tf.math.exp(x[-1:]),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "def S(x, w, v, b):\n",
    "    \"\"\"\n",
    "    x: scalar\n",
    "    w, v, b: (3, H)\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x, dtype=\"float64\")\n",
    "    # tf.debugging.assert_positive(x, message=\"R: x>0\")\n",
    "    exp_w_v = tf.math.exp([w, v])\n",
    "    ew = exp_w_v[0]\n",
    "    ev = exp_w_v[1]\n",
    "    # b = tf.math.sigmoid(b) # try this  # JT - bug. was sigb\n",
    "    ew = tf.concat([ew[:-1], tf.ones_like(ew[-1:]),], axis=0,)\n",
    "    x = tf.reshape(x, (1, 1))\n",
    "    return tf.transpose(ev) @ reloid(ew @ x + b)\n",
    "\n",
    "@tf.function\n",
    "def R(x, w, v, b):\n",
    "    return S(tf.math.log(x), w, v, b)\n",
    "\n",
    "@tf.function\n",
    "def Rinv(y, w, v, b):\n",
    "    y = tf.convert_to_tensor(y, dtype=\"float64\")\n",
    "    # y = tf.reshape(y, (-1,))[0]\n",
    "    # as x -> oo, R is asymyptotic to exp(v[-1] + w[-1]) x\n",
    "    # fixme: calculate this exactly.\n",
    "    x_left = tf.convert_to_tensor([[0.0]], tf.float64)\n",
    "    x_right = tf.convert_to_tensor([[1e8]], tf.float64)\n",
    "    # tf.print((x_left, x_right))\n",
    "    # tf.print(\"y\", y)\n",
    "    # tf.print('y',y)\n",
    "    # tf.debugging.assert_greater(R(x_right, w, v, b), y, message=\"R(x_right)>y inv\")\n",
    "\n",
    "    def cond(xl, xr):\n",
    "        # tf.print(xl, xr)\n",
    "        xi = (xl + xr) / 2.0\n",
    "        yi = R(xi, w, v, b)[0, 0]\n",
    "        return abs(y - yi) > 1e-6\n",
    "\n",
    "    def body(xl, xr):\n",
    "        xi = (xl + xr) / 2.0\n",
    "        yi = R(xi, w, v, b)[0, 0]\n",
    "        left = tf.cast(yi < y, dtype=\"float64\")\n",
    "        xl = left * xi + (1.0 - left) * xl\n",
    "        xr = (1.0 - left) * xi + left * xr\n",
    "        return (xl, xr)\n",
    "        # print(y, x_i, y_i)\n",
    "\n",
    "    xl, xr = tf.while_loop(cond, body, (x_left, x_right))\n",
    "    return (xl + xr) / 2.0\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_Rinv(y, w, v, b):\n",
    "    y = tf.convert_to_tensor(y, dtype=\"float64\")\n",
    "    x = Rinv(y, w, v, b)\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch([x, w, v, b])\n",
    "        y = R(x, w, v, b)\n",
    "    dR_dw, dR_dv, dR_db, dR_dx = g.gradient(y, [w, v, b, x])\n",
    "\n",
    "    def grad(dx):\n",
    "        return dx / dR_dx, -dx * dR_dw / dR_dx, -dx * dR_dv / dR_dx, -dx * dR_db / dR_dx\n",
    "\n",
    "    return x, grad\n",
    "\n",
    "mu = 1e-4\n",
    "rho = 1e-5\n",
    "\n",
    "def _gen_gaps(k: int, _R, _Rinv,) -> tf.Tensor:\n",
    "    \"\"\"Return k gaps sampled from genetic distribution with rate function eta.\"\"\"\n",
    "    z = tf.convert_to_tensor([[rexp()]])\n",
    "    x = _Rinv(z)  # initialize x by sampling from prior\n",
    "    tf.debugging.assert_positive(x, message=\"gen_gaps first x\")\n",
    "\n",
    "    gap = tf.constant([[0.0]], dtype=tf.float64)\n",
    "    j = 0\n",
    "    ta = tf.TensorArray(tf.float64, size=k + 1)\n",
    "\n",
    "    while tf.less(j, k + 1):\n",
    "        # x' satisfies R(x') - R(u*x) = Z => x' = Rinv(Z + R(u*x))\n",
    "        u = runif()\n",
    "        z = rexp()\n",
    "        u_x = tf.convert_to_tensor([[u * x]])\n",
    "        r_u_x = _R(u_x)  # compute R(u_x)\n",
    "        x = _Rinv(z + r_u_x)  # segment height\n",
    "        # tf.print(x)\n",
    "        # tf.print(z+r_u_x,\"\\n\")\n",
    "        with tf.control_dependencies(\n",
    "            [\n",
    "                tf.debugging.assert_all_finite(x, \"second x\"),\n",
    "                tf.debugging.assert_positive(x, message=\"gen_gaps second x\"),\n",
    "            ]\n",
    "        ):\n",
    "            next_event = rexp() / (x * (mu + rho))\n",
    "            tf.debugging.assert_positive(next_event, message=\"gen_gaps first gap\")\n",
    "            gap += next_event  # length to next event\n",
    "        while runif() < (mu / (mu + rho)) and tf.less(j, k + 1):\n",
    "            ta = ta.write(j, gap)\n",
    "            gap *= 0.0\n",
    "            j += 1\n",
    "            next_event = rexp() / (x * (mu + rho))\n",
    "            tf.debugging.assert_positive(next_event, message=\"gen_gaps second gap\")\n",
    "            gap += next_event  # length to next event\n",
    "\n",
    "    gaps = ta.stack()[1:]  # first obs suffers from inspection paradox?\n",
    "    with tf.control_dependencies(\n",
    "        [\n",
    "            tf.debugging.assert_positive(\n",
    "                gaps, message=\"gaps have non-positive entry\", summarize=100\n",
    "            )\n",
    "        ]\n",
    "    ):\n",
    "        return gaps\n",
    "\n",
    "\n",
    "def R_learned(x, generator):\n",
    "    return R(x, generator.weights[0], generator.weights[1], generator.weights[2])\n",
    "\n",
    "thresh = tf.constant([1e-1], dtype=\"float64\", shape=(1,))\n",
    "\n",
    "def eta(x):\n",
    "    x = tf.cast(x, \"float64\")\n",
    "    one = tf.ones(shape=[1,], dtype=\"float64\",)\n",
    "    return tf.cast(tf.where(x < thresh, 1 / 100, one), \"float64\")\n",
    "\n",
    "def R_real(x):\n",
    "    \"\"\"R_real(x) = integral_0^x eta(t) dt\"\"\"\n",
    "    x = tf.cast(x, \"float64\")\n",
    "    x = tf.reshape(x, (1, tf.size(x)))\n",
    "    tf.debugging.assert_positive(x, message=\"R_real: x>0\")\n",
    "    return tf.cast(\n",
    "        tf.where(x < thresh, x / 100.0, thresh / 100.0 + (x - thresh)), \"float64\"\n",
    "    )\n",
    "    # return x\n",
    "\n",
    "\n",
    "def R_real_inv(y):\n",
    "    y = tf.cast(y, \"float64\")\n",
    "    tf.debugging.assert_positive(y, message=\"R_real: x>0\")\n",
    "    return tf.cast(\n",
    "        tf.where(y < thresh / 100.0, y * 100.0, y - (thresh / 100.0 - thresh)),\n",
    "        \"float64\",\n",
    "    )\n",
    "\n",
    "@tf.function\n",
    "def gen_gaps(\n",
    "    w, v, b, k,\n",
    "):\n",
    "    R_ = lambda x: R(x, w, v, b)\n",
    "    Rinv_ = lambda z: custom_Rinv(z, w, v, b)\n",
    "    return _gen_gaps(k, R_, Rinv_)\n",
    "\n",
    "@tf.function\n",
    "def gen_gaps_real(k: int,):\n",
    "    return _gen_gaps(k, R_real, R_real_inv,)\n",
    "\n",
    "def discriminator_objective(d_x, g_z):\n",
    "    \"\"\"\n",
    "    d_x = real output\n",
    "    g_z = fake output\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(\n",
    "        tf.ones_like(d_x), d_x\n",
    "    )  # If we feed the discriminator with real images, we assume they all are the right pictures --> Because of that label == 1\n",
    "    fake_loss = cross_entropy(\n",
    "        tf.zeros_like(g_z), g_z\n",
    "    )  # Each noise we feed in are fakes image --> Because of that labels are 0\n",
    "    total_loss = real_loss + fake_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ke3db-pVL6Sj"
   },
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy()\n",
    "@tf.function\n",
    "def training_step(discriminator):\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # both fake and real seq have shape [Batch_size, seq_len, 1]\n",
    "        fake_seq = tf.reshape(tf.stack([gen_gaps(w, v, b, seq_len) for _ in range(batch_size)]),[batch_size,seq_len,1])\n",
    "        real_seq = tf.reshape(tf.stack([tf.reshape(gen_gaps_real(seq_len), (1, seq_len))\\\n",
    "                                      for _ in range(batch_size)]),[batch_size,seq_len,1])\n",
    "        d_x_true = discriminator(tf.expand_dims(tf.math.log(real_seq),-1))\n",
    "        d_x_fake = discriminator(tf.expand_dims(tf.math.log(fake_seq),-1))\n",
    "        \n",
    "        discriminator_loss = discriminator_objective(d_x_true, d_x_fake)\n",
    "        # Adjusting Gradient of Discriminator\n",
    "        gradients_of_discriminator = disc_tape.gradient(\n",
    "            discriminator_loss, discriminator.trainable_variables\n",
    "        )\n",
    "        discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "        )  # Takes a list of gradient and variables pairs\n",
    "    m.reset_states()\n",
    "    m.update_state(tf.ones_like(d_x_true),tf.math.sigmoid(d_x_true))\n",
    "    real_acc = m.result()\n",
    "\n",
    "    m.reset_states()\n",
    "    m.update_state(tf.zeros_like(d_x_fake),tf.math.sigmoid(d_x_fake))\n",
    "    fake_acc = m.result()\n",
    "\n",
    "    return discriminator_loss, real_acc, fake_acc\n",
    "  \n",
    "def training(epoches):\n",
    "    for epoch in range(epoches + 1):\n",
    "        disc_loss,real,fake = training_step(discriminator)\n",
    "        d_loss.append(disc_loss)\n",
    "        real_acc.append(real)\n",
    "        fake_acc.append(fake)\n",
    "        print(\"epoch=%d discriminator_loss=%f real_acc=%f fake_acc=%f\"\n",
    "                % (epoch, disc_loss, real, fake))\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S9jhuPJK-703"
   },
   "outputs": [],
   "source": [
    "seq_len = 500\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "EPOCHES = 500\n",
    "discriminator_optimizer = optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ec5wj8J8NNHq"
   },
   "outputs": [],
   "source": [
    "#upload weights.csv to colab\n",
    "w,v,b=np.loadtxt('weights.csv',delimiter=',')\n",
    "w = tf.expand_dims(w,-1)\n",
    "v = tf.expand_dims(v,-1)\n",
    "b = tf.expand_dims(b,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "NWDNT8MzOec_",
    "outputId": "6c1e1581-2078-4046-b458-927f33c89271"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = tf.math.log(gen_gaps(w, v, b, 1000))\n",
    "y = tf.math.log(gen_gaps_real(1000))\n",
    "plt.hist(x.numpy().reshape(-1),label = 'fake')\n",
    "plt.hist(y.numpy().reshape(-1), alpha=0.4,label='real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = gen_gaps(w, v, b, 1000)\n",
    "y = gen_gaps_real(1000)\n",
    "plt.hist(x.numpy().reshape(-1),label = 'fake')\n",
    "plt.hist(y.numpy().reshape(-1), alpha=0.4,label='real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "LN8jwzyXLxoW",
    "outputId": "d0b2b7c7-d9f1-42f8-d287-842ec8fce0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 497, 256)          1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 497, 256)          1024      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 497, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 248, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 248, 128)          32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 248, 64)           8256      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 15873     \n",
      "=================================================================\n",
      "Total params: 59,329\n",
      "Trainable params: 58,817\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential(\n",
    "    [\n",
    "     Input(shape=(seq_len,1)),\n",
    "     Conv1D(filters=256, kernel_size=4),\n",
    "     BatchNormalization(),\n",
    "     ReLU(),\n",
    "     MaxPooling1D(),\n",
    "     Dense(128),\n",
    "     Dense(64),\n",
    "     Flatten(),\n",
    "     Dense(1)\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3XQB77QPNFVt",
    "outputId": "ea604227-c9bc-4685-8265-ad60c7e63bcd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 discriminator_loss=1.381931 real_acc=0.790000 fake_acc=0.280000\n",
      "epoch=1 discriminator_loss=95.305695 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=2 discriminator_loss=379.108917 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=3 discriminator_loss=62.447243 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=4 discriminator_loss=144.265259 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=5 discriminator_loss=69.204132 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=6 discriminator_loss=45.347794 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=7 discriminator_loss=54.051125 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=8 discriminator_loss=25.588306 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=9 discriminator_loss=3.492290 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=10 discriminator_loss=6.943209 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=11 discriminator_loss=8.101771 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=12 discriminator_loss=6.882767 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=13 discriminator_loss=4.232805 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=14 discriminator_loss=1.449604 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=15 discriminator_loss=3.253203 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=16 discriminator_loss=4.934463 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=17 discriminator_loss=4.837174 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=18 discriminator_loss=3.171087 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=19 discriminator_loss=1.386019 real_acc=0.990000 fake_acc=0.040000\n",
      "epoch=20 discriminator_loss=3.035442 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=21 discriminator_loss=4.046425 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=22 discriminator_loss=3.383430 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=23 discriminator_loss=1.663098 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=24 discriminator_loss=3.258946 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=25 discriminator_loss=2.923767 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=26 discriminator_loss=2.357925 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=27 discriminator_loss=1.398858 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=28 discriminator_loss=2.055790 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=29 discriminator_loss=2.470689 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=30 discriminator_loss=1.821004 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=31 discriminator_loss=1.443045 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=32 discriminator_loss=2.075951 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=33 discriminator_loss=2.011997 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=34 discriminator_loss=1.424593 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=35 discriminator_loss=1.678575 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=36 discriminator_loss=1.949620 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=37 discriminator_loss=1.556886 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=38 discriminator_loss=1.455150 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=39 discriminator_loss=1.786684 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=40 discriminator_loss=1.618744 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=41 discriminator_loss=1.390077 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=42 discriminator_loss=1.634226 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=43 discriminator_loss=1.616580 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=44 discriminator_loss=1.390292 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=45 discriminator_loss=1.527352 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=46 discriminator_loss=1.582510 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=47 discriminator_loss=1.402409 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=48 discriminator_loss=1.464870 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=49 discriminator_loss=1.540605 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=50 discriminator_loss=1.409595 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=51 discriminator_loss=1.430241 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=52 discriminator_loss=1.502870 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=53 discriminator_loss=1.410009 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=54 discriminator_loss=1.412833 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=55 discriminator_loss=1.473073 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=56 discriminator_loss=1.406418 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=57 discriminator_loss=1.403925 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=58 discriminator_loss=1.450405 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=59 discriminator_loss=1.401228 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=60 discriminator_loss=1.399771 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=61 discriminator_loss=1.433623 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=62 discriminator_loss=1.396075 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=63 discriminator_loss=1.397918 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=64 discriminator_loss=1.421238 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=65 discriminator_loss=1.391691 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=66 discriminator_loss=1.397056 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=67 discriminator_loss=1.411304 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=68 discriminator_loss=1.388528 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=69 discriminator_loss=1.396784 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=70 discriminator_loss=1.403942 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=71 discriminator_loss=1.386870 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=72 discriminator_loss=1.396454 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=76 discriminator_loss=1.392789 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=77 discriminator_loss=1.386690 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=78 discriminator_loss=1.394508 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=79 discriminator_loss=1.389335 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=80 discriminator_loss=1.387608 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=81 discriminator_loss=1.392598 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=82 discriminator_loss=1.387234 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=83 discriminator_loss=1.388847 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=84 discriminator_loss=1.390588 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=85 discriminator_loss=1.386288 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=86 discriminator_loss=1.388729 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=87 discriminator_loss=1.388397 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=88 discriminator_loss=1.386413 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=89 discriminator_loss=1.388921 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=90 discriminator_loss=1.387068 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=91 discriminator_loss=1.387078 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=92 discriminator_loss=1.388236 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=93 discriminator_loss=1.386263 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=94 discriminator_loss=1.387560 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=95 discriminator_loss=1.387320 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=96 discriminator_loss=1.386466 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=97 discriminator_loss=1.387570 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=98 discriminator_loss=1.386568 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=99 discriminator_loss=1.386958 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=100 discriminator_loss=1.387055 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=101 discriminator_loss=1.386435 real_acc=0.000000 fake_acc=0.990000\n",
      "epoch=102 discriminator_loss=1.386980 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=103 discriminator_loss=1.386697 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=104 discriminator_loss=1.386633 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=105 discriminator_loss=1.386762 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=106 discriminator_loss=1.386371 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=107 discriminator_loss=1.386644 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=108 discriminator_loss=1.386571 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=109 discriminator_loss=1.386406 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=110 discriminator_loss=1.386641 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=111 discriminator_loss=1.386385 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=112 discriminator_loss=1.386434 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=113 discriminator_loss=1.386455 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=114 discriminator_loss=1.386189 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=115 discriminator_loss=1.386383 real_acc=0.000000 fake_acc=1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=116 discriminator_loss=1.386263 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=117 discriminator_loss=1.386349 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=118 discriminator_loss=1.386351 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=119 discriminator_loss=1.386376 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=120 discriminator_loss=1.386351 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=121 discriminator_loss=1.386330 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=122 discriminator_loss=1.386413 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=123 discriminator_loss=1.386341 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=124 discriminator_loss=1.386241 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=125 discriminator_loss=1.386358 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=126 discriminator_loss=1.386291 real_acc=0.000000 fake_acc=0.990000\n",
      "epoch=127 discriminator_loss=1.386151 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=128 discriminator_loss=1.386332 real_acc=0.990000 fake_acc=0.000000\n",
      "epoch=129 discriminator_loss=1.386249 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=130 discriminator_loss=1.386353 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=131 discriminator_loss=1.386481 real_acc=0.920000 fake_acc=0.030000\n",
      "epoch=132 discriminator_loss=1.386244 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=133 discriminator_loss=1.386287 real_acc=0.990000 fake_acc=0.020000\n",
      "epoch=134 discriminator_loss=1.386324 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=135 discriminator_loss=1.386270 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=136 discriminator_loss=1.386391 real_acc=0.980000 fake_acc=0.010000\n",
      "epoch=137 discriminator_loss=1.386360 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=138 discriminator_loss=1.386333 real_acc=0.850000 fake_acc=0.160000\n",
      "epoch=139 discriminator_loss=1.386359 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=140 discriminator_loss=1.386305 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=141 discriminator_loss=1.386394 real_acc=0.980000 fake_acc=0.010000\n",
      "epoch=142 discriminator_loss=1.386250 real_acc=0.990000 fake_acc=0.010000\n",
      "epoch=143 discriminator_loss=1.386428 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=144 discriminator_loss=1.386342 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=145 discriminator_loss=1.386246 real_acc=0.920000 fake_acc=0.130000\n",
      "epoch=146 discriminator_loss=1.386334 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=147 discriminator_loss=1.386323 real_acc=0.970000 fake_acc=0.030000\n",
      "epoch=148 discriminator_loss=1.386199 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=149 discriminator_loss=1.386364 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=150 discriminator_loss=1.385445 real_acc=0.990000 fake_acc=0.020000\n",
      "epoch=151 discriminator_loss=1.386440 real_acc=0.960000 fake_acc=0.010000\n",
      "epoch=152 discriminator_loss=1.386331 real_acc=0.000000 fake_acc=0.980000\n",
      "epoch=153 discriminator_loss=1.386322 real_acc=0.010000 fake_acc=1.000000\n",
      "epoch=154 discriminator_loss=1.386325 real_acc=0.870000 fake_acc=0.100000\n",
      "epoch=155 discriminator_loss=1.386405 real_acc=0.950000 fake_acc=0.030000\n",
      "epoch=156 discriminator_loss=1.386178 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=157 discriminator_loss=1.387014 real_acc=0.830000 fake_acc=0.250000\n",
      "epoch=158 discriminator_loss=1.509510 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=159 discriminator_loss=1.679469 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=160 discriminator_loss=1.464553 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=161 discriminator_loss=1.542582 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=162 discriminator_loss=1.563042 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=163 discriminator_loss=1.414186 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=164 discriminator_loss=1.580454 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=165 discriminator_loss=1.389855 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=166 discriminator_loss=1.527174 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=167 discriminator_loss=1.436640 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=168 discriminator_loss=1.446784 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=169 discriminator_loss=1.479328 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=170 discriminator_loss=1.393472 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=171 discriminator_loss=1.481447 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=172 discriminator_loss=1.390310 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=173 discriminator_loss=1.449654 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=174 discriminator_loss=1.414875 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=175 discriminator_loss=1.410063 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=176 discriminator_loss=1.433928 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=177 discriminator_loss=1.388177 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=178 discriminator_loss=1.432098 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=179 discriminator_loss=1.389511 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=180 discriminator_loss=1.414830 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=181 discriminator_loss=1.401848 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=182 discriminator_loss=1.396097 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=183 discriminator_loss=1.410117 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=184 discriminator_loss=1.386813 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=185 discriminator_loss=1.408206 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=186 discriminator_loss=1.388239 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=187 discriminator_loss=1.399645 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=188 discriminator_loss=1.394165 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=189 discriminator_loss=1.390884 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=190 discriminator_loss=1.397868 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=191 discriminator_loss=1.386499 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=192 discriminator_loss=1.396929 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=193 discriminator_loss=1.387013 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=194 discriminator_loss=1.393588 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=195 discriminator_loss=1.389758 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=196 discriminator_loss=1.388800 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=197 discriminator_loss=1.391604 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=198 discriminator_loss=1.386630 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=199 discriminator_loss=1.391380 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=200 discriminator_loss=1.386549 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=201 discriminator_loss=1.389909 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=202 discriminator_loss=1.387515 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=203 discriminator_loss=1.387815 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=204 discriminator_loss=1.388610 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=205 discriminator_loss=1.386549 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=206 discriminator_loss=1.388790 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=207 discriminator_loss=1.386354 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=208 discriminator_loss=1.388362 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=209 discriminator_loss=1.386306 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=210 discriminator_loss=1.387520 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=211 discriminator_loss=1.386541 real_acc=0.020000 fake_acc=0.950000\n",
      "epoch=212 discriminator_loss=1.427784 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=213 discriminator_loss=1.436004 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=214 discriminator_loss=1.398126 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=215 discriminator_loss=1.502458 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=216 discriminator_loss=1.405265 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=217 discriminator_loss=1.424420 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=218 discriminator_loss=1.389132 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=219 discriminator_loss=1.423936 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=220 discriminator_loss=1.386900 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=221 discriminator_loss=1.414159 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=222 discriminator_loss=1.392883 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=223 discriminator_loss=1.401302 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=224 discriminator_loss=1.399837 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=225 discriminator_loss=1.391273 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=226 discriminator_loss=1.402617 real_acc=0.000000 fake_acc=1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=227 discriminator_loss=1.386668 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=228 discriminator_loss=1.401253 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=229 discriminator_loss=1.386853 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=230 discriminator_loss=1.396668 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=231 discriminator_loss=1.389451 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=232 discriminator_loss=1.391779 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=233 discriminator_loss=1.391884 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=234 discriminator_loss=1.387916 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=235 discriminator_loss=1.392833 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=236 discriminator_loss=1.386433 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=237 discriminator_loss=1.392203 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=238 discriminator_loss=1.386423 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=239 discriminator_loss=1.390549 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=240 discriminator_loss=1.387486 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=241 discriminator_loss=1.388580 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=242 discriminator_loss=1.388318 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=243 discriminator_loss=1.387223 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=244 discriminator_loss=1.388631 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=245 discriminator_loss=1.386523 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=246 discriminator_loss=1.388578 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=247 discriminator_loss=1.386288 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=248 discriminator_loss=1.387919 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=249 discriminator_loss=1.386514 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=250 discriminator_loss=1.387525 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=251 discriminator_loss=1.388034 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=252 discriminator_loss=1.387009 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=253 discriminator_loss=1.386996 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=254 discriminator_loss=1.386491 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=255 discriminator_loss=1.386753 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=256 discriminator_loss=1.387233 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=257 discriminator_loss=1.387176 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=258 discriminator_loss=1.386451 real_acc=0.950000 fake_acc=0.040000\n",
      "epoch=259 discriminator_loss=1.386726 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=260 discriminator_loss=1.386272 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=261 discriminator_loss=1.386573 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=262 discriminator_loss=1.386319 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=263 discriminator_loss=1.386469 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=264 discriminator_loss=1.386344 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=265 discriminator_loss=1.386462 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=266 discriminator_loss=1.386381 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=267 discriminator_loss=1.386719 real_acc=0.990000 fake_acc=0.000000\n",
      "epoch=268 discriminator_loss=1.386412 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=269 discriminator_loss=1.386269 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=270 discriminator_loss=1.386495 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=271 discriminator_loss=1.386277 real_acc=0.930000 fake_acc=0.090000\n",
      "epoch=272 discriminator_loss=1.386400 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=273 discriminator_loss=1.386330 real_acc=0.990000 fake_acc=0.010000\n",
      "epoch=274 discriminator_loss=1.386384 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=275 discriminator_loss=1.386432 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=276 discriminator_loss=1.386366 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=277 discriminator_loss=1.386380 real_acc=0.990000 fake_acc=0.000000\n",
      "epoch=278 discriminator_loss=1.386296 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=279 discriminator_loss=1.386296 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=280 discriminator_loss=1.386370 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=281 discriminator_loss=1.386434 real_acc=0.990000 fake_acc=0.000000\n",
      "epoch=282 discriminator_loss=1.386285 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=283 discriminator_loss=1.386305 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=284 discriminator_loss=1.386191 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=285 discriminator_loss=1.386307 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=286 discriminator_loss=1.386208 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=287 discriminator_loss=1.386279 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=288 discriminator_loss=1.386559 real_acc=0.890000 fake_acc=0.130000\n",
      "epoch=289 discriminator_loss=1.386350 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=290 discriminator_loss=1.386302 real_acc=0.920000 fake_acc=0.070000\n",
      "epoch=291 discriminator_loss=1.386255 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=292 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=293 discriminator_loss=1.386273 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=294 discriminator_loss=1.386312 real_acc=0.980000 fake_acc=0.010000\n",
      "epoch=295 discriminator_loss=1.386364 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=296 discriminator_loss=1.386278 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=297 discriminator_loss=1.386410 real_acc=0.990000 fake_acc=0.000000\n",
      "epoch=298 discriminator_loss=1.386292 real_acc=0.990000 fake_acc=0.010000\n",
      "epoch=299 discriminator_loss=1.386100 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=300 discriminator_loss=1.386311 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=301 discriminator_loss=1.386304 real_acc=0.990000 fake_acc=0.000000\n",
      "epoch=302 discriminator_loss=1.386400 real_acc=0.970000 fake_acc=0.020000\n",
      "epoch=303 discriminator_loss=1.386274 real_acc=0.010000 fake_acc=1.000000\n",
      "epoch=304 discriminator_loss=1.386300 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=305 discriminator_loss=1.386559 real_acc=0.980000 fake_acc=0.060000\n",
      "epoch=306 discriminator_loss=1.386326 real_acc=0.990000 fake_acc=0.000000\n",
      "epoch=307 discriminator_loss=1.386260 real_acc=0.010000 fake_acc=1.000000\n",
      "epoch=308 discriminator_loss=1.386228 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=309 discriminator_loss=1.386406 real_acc=0.890000 fake_acc=0.070000\n",
      "epoch=310 discriminator_loss=1.386319 real_acc=0.980000 fake_acc=0.040000\n",
      "epoch=311 discriminator_loss=1.386327 real_acc=0.010000 fake_acc=0.990000\n",
      "epoch=312 discriminator_loss=1.386409 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=313 discriminator_loss=1.386281 real_acc=0.860000 fake_acc=0.100000\n",
      "epoch=314 discriminator_loss=1.386468 real_acc=0.960000 fake_acc=0.020000\n",
      "epoch=315 discriminator_loss=1.386241 real_acc=0.820000 fake_acc=0.160000\n",
      "epoch=316 discriminator_loss=1.386398 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=317 discriminator_loss=1.386582 real_acc=0.870000 fake_acc=0.130000\n",
      "epoch=318 discriminator_loss=1.386181 real_acc=0.880000 fake_acc=0.110000\n",
      "epoch=319 discriminator_loss=1.386931 real_acc=0.220000 fake_acc=0.710000\n",
      "epoch=320 discriminator_loss=1.386141 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=321 discriminator_loss=1.386384 real_acc=0.300000 fake_acc=0.750000\n",
      "epoch=322 discriminator_loss=1.388589 real_acc=0.340000 fake_acc=0.580000\n",
      "epoch=323 discriminator_loss=1.383403 real_acc=0.740000 fake_acc=0.380000\n",
      "epoch=324 discriminator_loss=2.994923 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=325 discriminator_loss=3.449357 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=326 discriminator_loss=3.136443 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=327 discriminator_loss=1.405803 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=328 discriminator_loss=2.869001 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=329 discriminator_loss=2.359526 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=330 discriminator_loss=1.500218 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=331 discriminator_loss=2.410636 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=332 discriminator_loss=1.842690 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=333 discriminator_loss=1.543384 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=334 discriminator_loss=2.100240 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=335 discriminator_loss=1.614068 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=336 discriminator_loss=1.519810 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=337 discriminator_loss=1.882308 real_acc=0.000000 fake_acc=1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=338 discriminator_loss=1.549297 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=339 discriminator_loss=1.452313 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=340 discriminator_loss=1.724735 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=341 discriminator_loss=1.534348 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=342 discriminator_loss=1.403271 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=343 discriminator_loss=1.600728 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=344 discriminator_loss=1.536934 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=345 discriminator_loss=1.386346 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=346 discriminator_loss=1.497505 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=347 discriminator_loss=1.526763 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=348 discriminator_loss=1.402610 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=349 discriminator_loss=1.423423 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=350 discriminator_loss=1.494110 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=351 discriminator_loss=1.431273 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=352 discriminator_loss=1.388636 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=353 discriminator_loss=1.446086 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=354 discriminator_loss=1.445642 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=355 discriminator_loss=1.391237 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=356 discriminator_loss=1.404666 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=357 discriminator_loss=1.435889 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=358 discriminator_loss=1.408137 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=359 discriminator_loss=1.386839 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=360 discriminator_loss=1.411504 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=361 discriminator_loss=1.415952 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=362 discriminator_loss=1.390826 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=363 discriminator_loss=1.391534 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=364 discriminator_loss=1.408302 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=365 discriminator_loss=1.399979 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=366 discriminator_loss=1.386420 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=367 discriminator_loss=1.395003 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=368 discriminator_loss=1.401363 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=369 discriminator_loss=1.389947 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=370 discriminator_loss=1.387064 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=371 discriminator_loss=1.395108 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=372 discriminator_loss=1.394689 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=373 discriminator_loss=1.387135 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=374 discriminator_loss=1.388439 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=375 discriminator_loss=1.391852 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=376 discriminator_loss=1.390151 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=377 discriminator_loss=1.386288 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=378 discriminator_loss=1.389731 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=379 discriminator_loss=1.390973 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=380 discriminator_loss=1.386348 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=381 discriminator_loss=1.386446 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=382 discriminator_loss=1.389206 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=383 discriminator_loss=1.388923 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=384 discriminator_loss=1.385097 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=385 discriminator_loss=1.387021 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=386 discriminator_loss=1.388459 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=387 discriminator_loss=1.385748 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=388 discriminator_loss=1.386360 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=389 discriminator_loss=1.385688 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=390 discriminator_loss=1.391751 real_acc=0.980000 fake_acc=0.000000\n",
      "epoch=391 discriminator_loss=1.386563 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=392 discriminator_loss=1.385377 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=393 discriminator_loss=1.387966 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=394 discriminator_loss=1.390748 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=395 discriminator_loss=1.388847 real_acc=0.960000 fake_acc=0.020000\n",
      "epoch=396 discriminator_loss=1.383536 real_acc=1.000000 fake_acc=0.060000\n",
      "epoch=397 discriminator_loss=1.389302 real_acc=0.990000 fake_acc=0.000000\n",
      "epoch=398 discriminator_loss=1.388778 real_acc=0.980000 fake_acc=0.010000\n",
      "epoch=399 discriminator_loss=1.387515 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=400 discriminator_loss=1.387005 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=401 discriminator_loss=1.385881 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=402 discriminator_loss=1.384382 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=403 discriminator_loss=1.387372 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=404 discriminator_loss=1.387352 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=405 discriminator_loss=1.386084 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=406 discriminator_loss=1.386703 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=407 discriminator_loss=1.392230 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=408 discriminator_loss=1.386456 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=409 discriminator_loss=1.386974 real_acc=0.990000 fake_acc=0.010000\n",
      "epoch=410 discriminator_loss=1.386668 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=411 discriminator_loss=1.386150 real_acc=1.000000 fake_acc=0.010000\n",
      "epoch=412 discriminator_loss=1.387874 real_acc=0.990000 fake_acc=0.000000\n",
      "epoch=413 discriminator_loss=1.386293 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=414 discriminator_loss=1.386592 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=415 discriminator_loss=1.386510 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=416 discriminator_loss=1.386515 real_acc=0.020000 fake_acc=0.930000\n",
      "epoch=417 discriminator_loss=1.386441 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=418 discriminator_loss=1.386604 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=419 discriminator_loss=1.386388 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=420 discriminator_loss=1.386423 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=421 discriminator_loss=1.386420 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=422 discriminator_loss=1.386161 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=423 discriminator_loss=1.386369 real_acc=0.000000 fake_acc=0.990000\n",
      "epoch=424 discriminator_loss=1.386372 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=425 discriminator_loss=1.386363 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=426 discriminator_loss=1.386360 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=427 discriminator_loss=1.386265 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=428 discriminator_loss=1.388713 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=429 discriminator_loss=1.386336 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=430 discriminator_loss=1.386343 real_acc=0.010000 fake_acc=0.970000\n",
      "epoch=431 discriminator_loss=1.386288 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=432 discriminator_loss=1.386375 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=433 discriminator_loss=1.386310 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=434 discriminator_loss=1.386269 real_acc=0.010000 fake_acc=1.000000\n",
      "epoch=435 discriminator_loss=1.386312 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=436 discriminator_loss=1.386327 real_acc=0.000000 fake_acc=0.990000\n",
      "epoch=437 discriminator_loss=1.386331 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=438 discriminator_loss=1.386334 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=439 discriminator_loss=1.386320 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=440 discriminator_loss=1.386352 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=441 discriminator_loss=1.386295 real_acc=0.020000 fake_acc=1.000000\n",
      "epoch=442 discriminator_loss=1.386360 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=443 discriminator_loss=1.386287 real_acc=0.000000 fake_acc=1.000000\n"
     ]
    }
   ],
   "source": [
    "d_loss = []\n",
    "real_acc = []\n",
    "fake_acc = []\n",
    "training(EPOCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "Ud8R_l_Gae02",
    "outputId": "cbfbfcf8-e5e9-4ca9-ae9a-0cda58f96f0c"
   },
   "outputs": [],
   "source": [
    "B = 20\n",
    "W = np.hamming(B)\n",
    "W /= W.sum()\n",
    "plt.figure(figsize = (21,8))\n",
    "plt.plot(range(len(real_acc)),np.convolve(real_acc, W, mode='same'),label='real')\n",
    "plt.plot(range(len(real_acc)),np.convolve(fake_acc, W, mode='same'),label='fake')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "train_D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
