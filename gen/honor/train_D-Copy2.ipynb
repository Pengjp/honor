{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rEP8Ti9V-1B4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, layers, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU,Conv1D,Dropout,MaxPooling1D,SimpleRNN,LocallyConnected1D\n",
    "from tensorflow.keras.layers import Flatten,ReLU, BatchNormalization,GlobalAveragePooling1D\n",
    "import numpy as np\n",
    "\n",
    "def runif():\n",
    "    return tf.random.uniform([1], dtype=tf.float64)[0]\n",
    "    # return tf.constant(.8, tf.float32)\n",
    "\n",
    "def rexp():\n",
    "    return -tf.math.log(runif())\n",
    "\n",
    "\n",
    "def exprelu(x):\n",
    "    return tf.where(x > 0, tf.math.expm1(x), tf.zeros_like(x))\n",
    "\n",
    "def reloid(x):\n",
    "    \"(sigma(x[1]), ..., sigma(x[-2]), relu(x[-1])\"\n",
    "    return tf.concat([tf.nn.sigmoid(x[:-1]), tf.math.exp(x[-1:])], axis=0)\n",
    "\n",
    "def reloid_derivative(x):\n",
    "    return tf.concat(\n",
    "        [\n",
    "            tf.nn.sigmoid(x[:-1])\n",
    "            * (1 - tf.nn.sigmoid(x[:-1])),  # derivative of sigmoid\n",
    "            tf.math.exp(x[-1:]),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "def S(x, w, v, b):\n",
    "    \"\"\"\n",
    "    x: scalar\n",
    "    w, v, b: (3, H)\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x, dtype=\"float64\")\n",
    "    # tf.debugging.assert_positive(x, message=\"R: x>0\")\n",
    "    exp_w_v = tf.math.exp([w, v])\n",
    "    ew = exp_w_v[0]\n",
    "    ev = exp_w_v[1]\n",
    "    # b = tf.math.sigmoid(b) # try this  # JT - bug. was sigb\n",
    "    ew = tf.concat([ew[:-1], tf.ones_like(ew[-1:]),], axis=0,)\n",
    "    x = tf.reshape(x, (1, 1))\n",
    "    return tf.transpose(ev) @ reloid(ew @ x + b)\n",
    "\n",
    "@tf.function\n",
    "def R(x, w, v, b):\n",
    "    return S(tf.math.log(x), w, v, b)\n",
    "\n",
    "@tf.function\n",
    "def Rinv(y, w, v, b):\n",
    "    y = tf.convert_to_tensor(y, dtype=\"float64\")\n",
    "    # y = tf.reshape(y, (-1,))[0]\n",
    "    # as x -> oo, R is asymyptotic to exp(v[-1] + w[-1]) x\n",
    "    # fixme: calculate this exactly.\n",
    "    x_left = tf.convert_to_tensor([[0.0]], tf.float64)\n",
    "    x_right = tf.convert_to_tensor([[1e8]], tf.float64)\n",
    "    # tf.print((x_left, x_right))\n",
    "    # tf.print(\"y\", y)\n",
    "    # tf.print('y',y)\n",
    "    # tf.debugging.assert_greater(R(x_right, w, v, b), y, message=\"R(x_right)>y inv\")\n",
    "\n",
    "    def cond(xl, xr):\n",
    "        # tf.print(xl, xr)\n",
    "        xi = (xl + xr) / 2.0\n",
    "        yi = R(xi, w, v, b)[0, 0]\n",
    "        return abs(y - yi) > 1e-6\n",
    "\n",
    "    def body(xl, xr):\n",
    "        xi = (xl + xr) / 2.0\n",
    "        yi = R(xi, w, v, b)[0, 0]\n",
    "        left = tf.cast(yi < y, dtype=\"float64\")\n",
    "        xl = left * xi + (1.0 - left) * xl\n",
    "        xr = (1.0 - left) * xi + left * xr\n",
    "        return (xl, xr)\n",
    "        # print(y, x_i, y_i)\n",
    "\n",
    "    xl, xr = tf.while_loop(cond, body, (x_left, x_right))\n",
    "    return (xl + xr) / 2.0\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_Rinv(y, w, v, b):\n",
    "    y = tf.convert_to_tensor(y, dtype=\"float64\")\n",
    "    x = Rinv(y, w, v, b)\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch([x, w, v, b])\n",
    "        y = R(x, w, v, b)\n",
    "    dR_dw, dR_dv, dR_db, dR_dx = g.gradient(y, [w, v, b, x])\n",
    "\n",
    "    def grad(dx):\n",
    "        return dx / dR_dx, -dx * dR_dw / dR_dx, -dx * dR_dv / dR_dx, -dx * dR_db / dR_dx\n",
    "\n",
    "    return x, grad\n",
    "\n",
    "mu = 1e-4\n",
    "rho = 1e-5\n",
    "\n",
    "def _gen_gaps(k: int, _R, _Rinv,) -> tf.Tensor:\n",
    "    \"\"\"Return k gaps sampled from genetic distribution with rate function eta.\"\"\"\n",
    "    z = tf.convert_to_tensor([[rexp()]])\n",
    "    x = _Rinv(z)  # initialize x by sampling from prior\n",
    "    tf.debugging.assert_positive(x, message=\"gen_gaps first x\")\n",
    "\n",
    "    gap = tf.constant([[0.0]], dtype=tf.float64)\n",
    "    j = 0\n",
    "    ta = tf.TensorArray(tf.float64, size=k + 1)\n",
    "\n",
    "    while tf.less(j, k + 1):\n",
    "        # x' satisfies R(x') - R(u*x) = Z => x' = Rinv(Z + R(u*x))\n",
    "        u = runif()\n",
    "        z = rexp()\n",
    "        u_x = tf.convert_to_tensor([[u * x]])\n",
    "        r_u_x = _R(u_x)  # compute R(u_x)\n",
    "        x = _Rinv(z + r_u_x)  # segment height\n",
    "        # tf.print(x)\n",
    "        # tf.print(z+r_u_x,\"\\n\")\n",
    "        with tf.control_dependencies(\n",
    "            [\n",
    "                tf.debugging.assert_all_finite(x, \"second x\"),\n",
    "                tf.debugging.assert_positive(x, message=\"gen_gaps second x\"),\n",
    "            ]\n",
    "        ):\n",
    "            next_event = rexp() / (x * (mu + rho))\n",
    "            tf.debugging.assert_positive(next_event, message=\"gen_gaps first gap\")\n",
    "            gap += next_event  # length to next event\n",
    "        while runif() < (mu / (mu + rho)) and tf.less(j, k + 1):\n",
    "            ta = ta.write(j, gap)\n",
    "            gap *= 0.0\n",
    "            j += 1\n",
    "            next_event = rexp() / (x * (mu + rho))\n",
    "            tf.debugging.assert_positive(next_event, message=\"gen_gaps second gap\")\n",
    "            gap += next_event  # length to next event\n",
    "\n",
    "    gaps = ta.stack()[1:]  # first obs suffers from inspection paradox?\n",
    "    with tf.control_dependencies(\n",
    "        [\n",
    "            tf.debugging.assert_positive(\n",
    "                gaps, message=\"gaps have non-positive entry\", summarize=100\n",
    "            )\n",
    "        ]\n",
    "    ):\n",
    "        return gaps\n",
    "\n",
    "\n",
    "def R_learned(x, generator):\n",
    "    return R(x, generator.weights[0], generator.weights[1], generator.weights[2])\n",
    "\n",
    "thresh = tf.constant([1e-1], dtype=\"float64\", shape=(1,))\n",
    "\n",
    "def eta(x):\n",
    "    x = tf.cast(x, \"float64\")\n",
    "    one = tf.ones(shape=[1,], dtype=\"float64\",)\n",
    "    return tf.cast(tf.where(x < thresh, 1 / 100, one), \"float64\")\n",
    "\n",
    "def R_real(x):\n",
    "    \"\"\"R_real(x) = integral_0^x eta(t) dt\"\"\"\n",
    "    x = tf.cast(x, \"float64\")\n",
    "    x = tf.reshape(x, (1, tf.size(x)))\n",
    "    tf.debugging.assert_positive(x, message=\"R_real: x>0\")\n",
    "    return tf.cast(\n",
    "        tf.where(x < thresh, x / 100.0, thresh / 100.0 + (x - thresh)), \"float64\"\n",
    "    )\n",
    "    # return x\n",
    "\n",
    "\n",
    "def R_real_inv(y):\n",
    "    y = tf.cast(y, \"float64\")\n",
    "    tf.debugging.assert_positive(y, message=\"R_real: x>0\")\n",
    "    return tf.cast(\n",
    "        tf.where(y < thresh / 100.0, y * 100.0, y - (thresh / 100.0 - thresh)),\n",
    "        \"float64\",\n",
    "    )\n",
    "\n",
    "@tf.function\n",
    "def gen_gaps(\n",
    "    w, v, b, k,\n",
    "):\n",
    "    R_ = lambda x: R(x, w, v, b)\n",
    "    Rinv_ = lambda z: custom_Rinv(z, w, v, b)\n",
    "    return _gen_gaps(k, R_, Rinv_)\n",
    "\n",
    "@tf.function\n",
    "def gen_gaps_real(k: int,):\n",
    "    return _gen_gaps(k, R_real, R_real_inv,)\n",
    "\n",
    "def discriminator_objective(d_x, g_z):\n",
    "    \"\"\"\n",
    "    d_x = real output\n",
    "    g_z = fake output\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(\n",
    "        tf.ones_like(d_x), d_x\n",
    "    )  # If we feed the discriminator with real images, we assume they all are the right pictures --> Because of that label == 1\n",
    "    fake_loss = cross_entropy(\n",
    "        tf.zeros_like(g_z), g_z\n",
    "    )  # Each noise we feed in are fakes image --> Because of that labels are 0\n",
    "    total_loss = real_loss + fake_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ke3db-pVL6Sj"
   },
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy()\n",
    "# @tf.function\n",
    "def training_step(discriminator):\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # both fake and real seq have shape [Batch_size, seq_len, 1]\n",
    "        fake_seq = tf.reshape(tf.stack([gen_gaps(w, v, b, seq_len) for _ in range(batch_size)]),[batch_size,seq_len,1])\n",
    "        real_seq = tf.reshape(tf.stack([tf.reshape(gen_gaps_real(seq_len), (1, seq_len))\\\n",
    "                                      for _ in range(batch_size)]),[batch_size,seq_len,1])\n",
    "        d_x_true = discriminator(tf.expand_dims(tf.math.log(real_seq),-1))\n",
    "        d_x_fake = discriminator(tf.expand_dims(tf.math.log(fake_seq),-1))\n",
    "        \n",
    "        discriminator_loss = discriminator_objective(d_x_true, d_x_fake)\n",
    "        # Adjusting Gradient of Discriminator\n",
    "        gradients_of_discriminator = disc_tape.gradient(\n",
    "            discriminator_loss, discriminator.trainable_variables\n",
    "        )\n",
    "        discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "        )  # Takes a list of gradient and variables pairs\n",
    "    m.reset_states()\n",
    "    m.update_state(tf.ones_like(d_x_true),tf.math.sigmoid(d_x_true))\n",
    "    real_acc = m.result()\n",
    "\n",
    "    m.reset_states()\n",
    "    m.update_state(tf.zeros_like(d_x_fake),tf.math.sigmoid(d_x_fake))\n",
    "    fake_acc = m.result()\n",
    "\n",
    "    return discriminator_loss, real_acc, fake_acc\n",
    "  \n",
    "def training(epoches):\n",
    "    for epoch in range(epoches + 1):\n",
    "        disc_loss,real,fake = training_step(discriminator)\n",
    "        d_loss.append(disc_loss)\n",
    "        real_acc.append(real)\n",
    "        fake_acc.append(fake)\n",
    "        print(\"epoch=%d discriminator_loss=%f real_acc=%f fake_acc=%f\"\n",
    "                % (epoch, disc_loss, real, fake))\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S9jhuPJK-703"
   },
   "outputs": [],
   "source": [
    "seq_len = 700\n",
    "learning_rate = 0.005\n",
    "batch_size = 100\n",
    "EPOCHES = 500\n",
    "discriminator_optimizer = optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ec5wj8J8NNHq"
   },
   "outputs": [],
   "source": [
    "#upload weights.csv to colab\n",
    "w,v,b=np.loadtxt('weights.csv',delimiter=',')\n",
    "w = tf.expand_dims(w,-1)\n",
    "v = tf.expand_dims(v,-1)\n",
    "b = tf.expand_dims(b,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "NWDNT8MzOec_",
    "outputId": "6c1e1581-2078-4046-b458-927f33c89271"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = tf.math.log(gen_gaps(w, v, b, 1000))\n",
    "y = tf.math.log(gen_gaps_real(1000))\n",
    "plt.hist(x.numpy().reshape(-1),label = 'fake',density=True,bins=50)\n",
    "plt.hist(y.numpy().reshape(-1), alpha=0.4,label='real',density=True,bins=50)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_seq = tf.reshape(tf.stack([gen_gaps(w, v, b, seq_len) for _ in range(batch_size)]),[batch_size,seq_len])\n",
    "real_seq = tf.reshape(tf.stack([tf.reshape(gen_gaps_real(seq_len), (1, seq_len))\\\n",
    "                              for _ in range(batch_size)]),[batch_size,seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(700,), dtype=float64, numpy=\n",
       "array([5.38659973e+02, 9.85428611e+02, 2.99709096e+03, 2.64483651e+04,\n",
       "       1.33038479e+04, 4.34097247e+03, 4.77984399e+04, 3.84985203e+03,\n",
       "       1.85779935e+04, 1.34898357e+04, 4.58468299e+04, 6.30736821e+03,\n",
       "       9.01569172e+03, 1.42153041e+04, 3.11409617e+03, 2.93308070e+03,\n",
       "       8.57778991e+02, 3.06172825e+03, 7.27565999e+03, 6.02243306e+03,\n",
       "       2.96112049e+02, 1.62498160e+03, 3.79714958e+02, 7.28675721e+03,\n",
       "       2.84627088e+03, 3.74024512e+02, 4.08527349e+02, 3.77135012e+03,\n",
       "       9.75283806e+02, 7.97537637e+03, 1.09614956e+03, 4.27977428e+03,\n",
       "       1.28222313e+04, 1.50323481e+03, 8.51397959e+02, 3.29230854e+03,\n",
       "       3.12791509e+03, 4.91545874e+02, 1.49425263e+03, 8.68204965e+03,\n",
       "       2.17365247e+03, 7.46519728e+03, 2.56241472e+03, 1.10095115e+03,\n",
       "       2.04211366e+03, 6.26871123e+03, 2.16295266e+02, 4.28084631e+01,\n",
       "       3.54362663e+03, 5.50345132e+02, 6.12020501e+02, 9.80222951e+03,\n",
       "       3.71759061e+00, 1.69364087e+03, 2.04268367e+03, 3.45427198e+02,\n",
       "       4.51974533e+02, 5.80847750e+02, 3.29017823e+02, 9.34992314e+02,\n",
       "       2.05662629e+03, 6.03413477e+02, 2.00482410e+03, 3.33713549e+02,\n",
       "       4.23293104e+03, 1.04762769e+02, 5.96671771e+03, 7.39020358e+02,\n",
       "       2.00846636e+03, 4.68475994e+03, 1.19112099e+03, 3.79853060e+02,\n",
       "       5.08749695e+02, 5.33773589e+03, 4.73081163e+02, 2.84253447e+04,\n",
       "       1.70612506e+04, 6.22998540e+03, 1.00991469e+04, 2.87957081e+04,\n",
       "       8.74633114e+03, 3.51507282e+03, 8.40957801e+03, 7.77698401e+03,\n",
       "       1.81337102e+04, 9.71003863e+03, 9.05917845e+02, 1.69668001e+04,\n",
       "       9.49748989e+03, 2.54509327e+03, 6.15976735e+02, 1.47914931e+04,\n",
       "       7.28114856e+04, 8.39479280e+04, 1.78865146e+03, 8.82089984e+02,\n",
       "       1.70555821e+04, 1.20981095e+04, 1.80328818e+03, 1.27020491e+04,\n",
       "       2.10552895e+03, 1.56659445e+02, 1.41765670e+04, 7.43449316e+03,\n",
       "       2.01216350e+03, 1.38859612e+03, 2.50192193e+03, 1.26743339e+03,\n",
       "       1.11358143e+03, 1.49968137e+02, 7.40299751e+02, 1.98517473e+03,\n",
       "       6.35511821e+03, 1.71154515e+02, 1.00477910e+04, 1.58983310e+03,\n",
       "       4.68363738e+03, 3.06669434e+03, 8.80472372e+03, 5.64488164e+04,\n",
       "       1.46958582e+05, 1.72102914e+05, 1.92936859e+05, 1.42697412e+03,\n",
       "       2.36709084e+03, 2.49600651e+03, 2.26911651e+02, 1.50225236e+04,\n",
       "       3.65402481e+03, 5.29548449e+01, 6.91507089e+02, 2.50234287e+03,\n",
       "       1.99473758e+03, 2.94570610e+03, 5.95729611e+03, 8.92014753e+03,\n",
       "       1.36418902e+03, 2.82579693e+03, 2.49125254e+02, 1.07530020e+03,\n",
       "       4.87427012e+00, 7.76328081e+02, 4.00158662e+02, 6.65868104e+02,\n",
       "       3.81239450e+03, 6.75230323e+01, 9.69650224e+02, 5.24763762e+02,\n",
       "       1.00264321e+03, 3.51051274e+03, 7.47716230e+03, 5.34489509e+03,\n",
       "       1.90973462e+01, 2.43683856e+03, 1.59728739e+02, 3.93101557e+03,\n",
       "       3.50096743e+02, 3.27729940e+03, 6.14227900e+03, 1.43621745e+03,\n",
       "       1.11065143e+04, 1.33113119e+04, 4.50217660e+02, 2.83704665e+03,\n",
       "       8.97688446e+01, 1.70405883e+04, 2.79375710e+03, 7.72101706e+02,\n",
       "       1.65557048e+03, 2.54301356e+03, 1.94179715e+02, 3.26120694e+03,\n",
       "       5.57020138e+03, 1.09950379e+03, 2.99013317e+03, 6.43009241e+02,\n",
       "       6.15865485e+03, 4.56797478e+01, 1.06421472e+03, 5.61640733e+02,\n",
       "       5.19367756e+03, 1.30878045e+03, 3.49004618e+03, 2.18310667e+03,\n",
       "       6.98422327e+03, 1.07615886e+03, 2.03921592e+03, 1.59318762e+03,\n",
       "       1.10827635e+03, 1.22119746e+03, 9.40555928e+03, 1.92138988e+03,\n",
       "       5.62948531e+03, 3.40090718e+03, 2.08691400e+03, 1.69841553e+03,\n",
       "       1.88952088e+03, 1.16974030e+04, 1.65420009e+02, 1.66437015e+02,\n",
       "       9.46805552e+00, 1.15656098e+03, 1.02159964e+03, 1.85696015e+01,\n",
       "       6.38037164e+02, 9.75729980e+00, 1.45742225e+03, 8.47839322e+00,\n",
       "       1.56780286e+03, 7.47081342e+01, 1.80727019e+02, 1.98832100e+02,\n",
       "       5.83855827e+01, 9.07455077e+02, 2.73089773e+02, 9.97892577e-01,\n",
       "       2.32431237e+02, 1.93385580e+02, 2.00180047e+02, 4.60388747e+02,\n",
       "       5.04009316e+02, 1.20233391e+02, 1.07112708e+02, 9.33069982e+01,\n",
       "       1.38385455e+02, 3.43277250e+01, 9.53155597e+01, 1.28089719e+02,\n",
       "       4.85478080e+02, 1.90864769e+03, 3.01174645e+02, 4.84934781e+01,\n",
       "       8.14589253e+02, 1.55387999e+01, 7.41917546e+01, 2.85698781e+02,\n",
       "       1.23397907e+03, 1.04874656e+02, 4.24654273e+02, 9.10007233e+01,\n",
       "       2.86163701e+02, 4.63710837e+02, 1.60756548e+01, 1.30981182e+02,\n",
       "       1.03211087e+02, 1.35594935e+00, 2.22953726e+03, 4.59958995e+03,\n",
       "       2.08461074e+02, 5.70883042e+02, 8.04757490e+02, 1.54455461e+02,\n",
       "       1.01597078e+03, 2.69424496e+02, 2.41577976e+03, 3.20933175e+02,\n",
       "       4.61034767e+02, 6.22134417e+03, 1.52698187e+01, 1.33437545e+02,\n",
       "       7.28785211e+02, 2.37815275e+03, 5.86812668e+02, 1.99425992e+02,\n",
       "       2.99895613e+02, 9.37032200e+02, 1.05320705e+03, 2.55723523e+02,\n",
       "       2.09768041e+03, 1.17632850e+03, 6.48134387e+02, 1.04631746e+02,\n",
       "       6.65290400e+02, 7.84081932e+00, 2.28318451e+02, 5.11176106e+02,\n",
       "       3.15991671e+02, 1.65493856e+04, 1.38058956e+03, 3.14097970e+01,\n",
       "       3.03737603e+03, 3.22436267e+03, 8.73687251e+02, 6.02030433e+03,\n",
       "       2.18658693e+03, 1.86581236e+04, 2.61232495e+02, 2.15456371e+03,\n",
       "       1.12649477e+04, 4.25709231e+03, 6.21349899e+02, 2.27398496e+03,\n",
       "       1.16498125e+04, 2.59578083e+03, 9.41854185e+03, 4.32235592e+03,\n",
       "       4.85131387e+03, 8.31117622e+03, 1.09060777e+03, 3.42921261e+03,\n",
       "       1.51845906e+03, 2.72031713e+03, 4.47791677e+02, 3.33714576e+01,\n",
       "       1.08296462e+03, 3.23846459e+03, 7.71777446e+03, 8.85905647e+03,\n",
       "       1.57760973e+00, 1.21274070e+04, 1.12737534e+03, 7.14500221e+03,\n",
       "       1.31173598e+04, 1.78570413e+03, 1.84944365e+03, 1.09302371e+04,\n",
       "       2.10284211e+04, 1.05845231e+03, 7.01565613e+03, 7.49656319e+03,\n",
       "       3.55130157e+03, 5.12689885e+02, 2.80527722e+02, 3.36515410e+03,\n",
       "       5.80804382e+03, 1.89912098e+01, 1.62398360e+03, 2.57201357e+01,\n",
       "       1.26236313e+03, 2.25227314e+03, 7.02489322e+03, 3.61909372e+03,\n",
       "       6.69334202e+02, 4.64522137e+01, 2.14537943e+04, 4.43538855e+03,\n",
       "       2.94850338e+03, 1.13151618e+04, 1.17860622e+02, 2.85849785e+03,\n",
       "       1.18752666e+02, 5.57257122e+02, 3.29702948e+03, 1.47978612e+04,\n",
       "       1.85882009e+03, 1.97338565e+03, 4.49023960e+03, 3.64375312e+03,\n",
       "       8.66463778e+03, 1.36943032e+03, 3.41112164e+04, 3.09673347e+03,\n",
       "       1.36653858e+04, 2.82530289e+03, 9.05373479e+03, 1.73021899e+03,\n",
       "       4.69623975e+02, 3.94835443e+03, 1.20325079e+03, 9.28426491e+02,\n",
       "       3.26540799e+02, 2.63782194e+02, 2.48935058e+02, 1.67290423e+01,\n",
       "       1.11760649e+01, 1.50778941e+03, 3.85651359e+03, 4.95855243e+02,\n",
       "       2.64336819e+02, 4.67076354e+02, 2.57302520e+02, 5.86997028e+01,\n",
       "       1.42852540e+03, 1.28283277e+04, 1.06338769e+02, 1.95732359e+03,\n",
       "       3.33578219e+03, 1.74516195e+03, 2.38050716e+02, 7.28826690e+02,\n",
       "       9.18348489e+00, 1.53561785e+03, 3.10246556e+02, 6.74594967e+03,\n",
       "       2.26110578e+03, 5.89377360e+03, 4.67966012e+03, 2.02924072e+03,\n",
       "       7.92178512e+02, 5.62506601e+02, 1.23432768e+03, 5.67072153e+03,\n",
       "       1.85010006e+03, 6.21653100e+01, 8.05784051e-01, 3.69143601e+02,\n",
       "       2.62431154e+02, 1.90444387e+03, 6.85135090e+02, 5.37763398e+03,\n",
       "       1.49895198e+03, 1.81255263e+02, 1.39784693e+03, 2.24170887e+02,\n",
       "       1.31270496e+03, 3.69564983e+02, 6.15422002e+03, 4.46016331e+02,\n",
       "       1.45192013e+03, 3.79686155e+02, 2.62692080e+03, 8.22746341e+03,\n",
       "       1.33081263e+04, 7.22456824e+03, 2.10137732e+04, 5.68175292e+03,\n",
       "       1.21989452e+04, 2.50914217e+03, 1.20066105e+03, 1.31954525e+02,\n",
       "       4.38261678e+03, 1.39402502e+04, 4.81607569e+03, 5.74275839e+03,\n",
       "       8.99498630e+03, 1.00024261e+04, 1.87031743e+04, 2.13153632e+03,\n",
       "       5.14608666e+03, 2.68892579e+04, 1.15627495e+04, 1.86777708e+04,\n",
       "       5.72825530e+03, 2.50783944e+02, 6.61857573e+03, 1.68613255e+04,\n",
       "       8.72523652e+03, 2.00634301e+03, 5.37889458e+02, 2.94852104e+02,\n",
       "       4.46983632e+01, 2.51257425e+02, 3.14209668e+03, 1.23514594e+01,\n",
       "       1.56466878e+03, 1.48372682e+01, 6.70200562e+03, 1.89407610e+02,\n",
       "       9.58917640e+02, 2.47121822e+02, 4.90747149e+03, 3.85014899e+03,\n",
       "       2.40875189e+03, 1.73668334e+02, 7.94554328e+01, 1.52008353e+02,\n",
       "       7.31182774e+03, 3.35303106e+03, 5.50634460e+04, 8.96442881e+03,\n",
       "       6.36235552e+03, 1.25997664e+02, 2.73636486e+04, 2.08300785e+04,\n",
       "       9.79638945e+03, 2.72486045e+03, 1.10057115e+03, 3.08014294e+01,\n",
       "       9.37271363e+02, 3.17227328e+02, 1.86918853e+03, 4.33355974e+01,\n",
       "       3.71066503e+01, 6.93470481e+02, 1.91040330e+02, 4.46283509e+02,\n",
       "       1.17982309e+03, 1.98387072e+03, 8.90864639e+03, 5.76159829e+02,\n",
       "       4.04908815e+02, 5.45492738e+02, 7.40317945e+02, 7.12662608e+02,\n",
       "       1.63060070e+03, 7.24037555e+03, 1.88465632e+03, 5.32573787e+02,\n",
       "       4.89910628e+02, 1.23276103e+00, 1.42874027e+03, 2.77525240e+02,\n",
       "       1.36516483e+03, 5.93246012e+02, 1.61580748e+02, 3.63868195e+03,\n",
       "       7.37969909e+03, 1.52965446e+02, 8.05508907e+02, 1.93370159e+03,\n",
       "       1.47369180e+03, 9.28287662e+02, 1.05472057e+03, 2.95127745e+02,\n",
       "       1.35547456e+03, 9.40095116e+01, 7.35592432e+02, 3.74220197e+02,\n",
       "       5.30260964e+02, 9.91878860e+02, 1.17661262e+03, 5.01183612e+03,\n",
       "       2.40271986e+03, 8.88930249e+01, 3.53385403e+03, 4.28024664e+02,\n",
       "       4.31617658e+02, 4.17652507e+02, 9.56938714e+01, 8.09607878e+00,\n",
       "       5.48488711e+02, 1.25805119e+03, 7.82863375e+03, 9.17200183e+02,\n",
       "       2.73713281e+02, 2.95406289e+03, 4.12448025e+02, 3.79306352e+03,\n",
       "       1.08648092e+03, 1.01032949e+03, 1.50683088e+03, 3.15729825e+02,\n",
       "       2.25905131e+03, 1.19429479e+03, 2.01296131e+02, 5.63220141e+01,\n",
       "       7.49110960e+02, 1.76766411e+03, 4.75769416e+02, 2.92369169e+02,\n",
       "       2.82936639e+03, 4.71091409e-01, 1.54070887e+02, 1.07583971e+03,\n",
       "       4.77083589e+03, 5.30735255e+03, 7.30401684e+02, 7.53288860e+00,\n",
       "       2.50607579e+02, 1.83542519e+03, 7.86117498e+01, 2.61449108e+03,\n",
       "       3.90015917e+02, 3.12679618e+03, 7.24315245e+02, 9.48343649e+03,\n",
       "       5.87476749e+02, 3.41896963e+03, 1.57366870e+03, 9.34202006e+01,\n",
       "       1.72842469e+03, 1.01897845e+02, 4.80485593e+02, 1.12830935e+03,\n",
       "       4.93485307e+02, 4.99013962e+01, 7.50110259e+03, 1.46386854e+03,\n",
       "       2.12544436e+03, 2.32983225e+03, 3.31324927e+02, 2.40078961e+02,\n",
       "       1.28999011e+03, 3.10442421e+02, 1.07673136e+03, 4.71141012e+02,\n",
       "       1.69350746e+02, 2.16347235e+02, 7.31736098e+01, 6.43227503e+02,\n",
       "       1.11052884e+02, 1.37146888e+02, 2.68286909e+03, 1.18151857e+03,\n",
       "       2.02922619e+02, 1.15869660e+03, 1.32894711e+03, 3.42818641e+03,\n",
       "       2.55287599e+02, 1.58484583e+02, 1.43555667e+03, 1.08772800e+02,\n",
       "       6.07340352e+03, 1.71356044e+04, 1.41395713e+04, 3.68011642e+03,\n",
       "       1.22610711e+03, 2.47336634e+03, 2.33076285e+02, 1.18669111e+04,\n",
       "       2.14991432e+03, 6.70574050e+03, 9.50013467e+01, 6.03986076e+01,\n",
       "       5.06634052e+03, 8.64041383e+03, 2.24335771e+03, 9.11267560e+01,\n",
       "       2.92187433e+03, 6.13438471e+03, 2.32482675e+01, 4.89041780e+01,\n",
       "       9.39607871e+01, 1.43724174e+03, 5.67848560e+02, 3.50126140e+02,\n",
       "       5.03448674e+02, 3.44843055e+02, 4.62276264e+02, 2.95656511e+01,\n",
       "       5.69580899e+00, 2.54314850e+03, 6.37523183e+03, 1.54259628e+03,\n",
       "       1.80460632e+04, 3.53394987e+03, 1.27417403e+03, 2.05437053e+03,\n",
       "       6.49613083e+01, 5.15261499e+02, 2.97063229e+02, 2.82472779e+03,\n",
       "       7.80308272e+02, 2.56305109e+04, 1.70579496e+04, 1.48946430e+02,\n",
       "       9.62319316e+03, 2.29244319e+03, 6.55934495e+03, 1.07686981e+03,\n",
       "       3.89689441e+04, 8.62866666e+03, 2.37460413e+03, 1.78995584e+03,\n",
       "       2.27143134e+03, 7.87219414e+03, 6.68130762e+02, 2.58902875e+03,\n",
       "       3.89307589e+02, 3.65788968e+03, 2.29138730e+03, 3.01544259e+02,\n",
       "       2.68037439e+03, 6.19445076e+03, 5.00836514e+03, 3.78934443e+02,\n",
       "       2.08751087e+03, 8.99793646e+03, 3.43879942e+02, 2.78732300e+02,\n",
       "       3.24884304e+02, 1.27538324e+03, 6.53821971e+02, 2.12882673e+03,\n",
       "       1.01467072e+03, 3.86759998e+02, 1.09274063e+03, 2.79339692e+02,\n",
       "       5.81002989e+03, 1.74401514e+03, 3.04645369e+03, 8.60933089e+02,\n",
       "       1.11794670e+04, 1.25880746e+03, 8.68963443e+03, 7.92741170e+03,\n",
       "       7.69640429e+02, 4.66998806e+03, 5.49324619e+02, 8.42364389e+01,\n",
       "       1.26002192e+03, 1.07162646e+04, 1.80705301e+03, 1.90366277e+03,\n",
       "       3.60516443e+03, 6.27784028e+03, 2.19208768e+03, 6.03089422e+04,\n",
       "       6.10961341e+02, 1.48428507e+04, 5.89988742e+01, 2.76062131e+04,\n",
       "       5.56900934e+03, 7.58678278e+04, 1.84018749e+04, 1.44553756e+04,\n",
       "       2.00914436e+04, 1.64926872e+04, 1.08635597e+04, 1.18162101e+04])>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.math.multiply (tf.random.uniform([seq_len,], dtype=tf.float64),fake_seq[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = gen_gaps(w, v, b, 1000)\n",
    "y = gen_gaps_real(1000)\n",
    "plt.hist(x.numpy().reshape(-1),label = 'fake')\n",
    "plt.hist(y.numpy().reshape(-1), alpha=0.4,label='real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "LN8jwzyXLxoW",
    "outputId": "d0b2b7c7-d9f1-42f8-d287-842ec8fce0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "locally_connected1d (Locally (None, 697, 128)          446080    \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 697, 128)          0         \n",
      "_________________________________________________________________\n",
      "locally_connected1d_1 (Local (None, 694, 64)           22785408  \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 694, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 44416)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 44417     \n",
      "=================================================================\n",
      "Total params: 23,275,905\n",
      "Trainable params: 23,275,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential(\n",
    "    [\n",
    "     Input(shape=(seq_len,1)),\n",
    "        \n",
    "#      LocallyConnected1D(128, 4),\n",
    "#      BatchNormalization(),\n",
    "#      ReLU(),\n",
    "#      MaxPooling1D(),\n",
    "        \n",
    "#      LocallyConnected1D(64, 4),\n",
    "#      ReLU(),\n",
    "#      MaxPooling1D(),\n",
    "#      BatchNormalization(),\n",
    "#      Dense(30),\n",
    "     Flatten(),\n",
    "     Dense(1)\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3XQB77QPNFVt",
    "outputId": "ea604227-c9bc-4685-8265-ad60c7e63bcd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 discriminator_loss=1.386240 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=1 discriminator_loss=1.856186 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=2 discriminator_loss=17.059341 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=3 discriminator_loss=3.153551 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=4 discriminator_loss=2.455318 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=5 discriminator_loss=1.633251 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=6 discriminator_loss=1.394255 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=7 discriminator_loss=1.395626 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=8 discriminator_loss=1.397464 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=9 discriminator_loss=1.388510 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=10 discriminator_loss=1.386440 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=11 discriminator_loss=1.389910 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=12 discriminator_loss=1.386224 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=13 discriminator_loss=1.387424 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=14 discriminator_loss=1.386930 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=15 discriminator_loss=1.384808 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=16 discriminator_loss=1.387132 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=17 discriminator_loss=1.386731 real_acc=0.000000 fake_acc=0.980000\n",
      "epoch=18 discriminator_loss=1.386130 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=19 discriminator_loss=1.383514 real_acc=1.000000 fake_acc=0.030000\n",
      "epoch=20 discriminator_loss=1.385761 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=21 discriminator_loss=1.385405 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=22 discriminator_loss=1.383268 real_acc=0.710000 fake_acc=0.480000\n",
      "epoch=23 discriminator_loss=1.383724 real_acc=0.990000 fake_acc=0.050000\n",
      "epoch=24 discriminator_loss=1.384778 real_acc=0.060000 fake_acc=0.920000\n",
      "epoch=25 discriminator_loss=1.381753 real_acc=0.010000 fake_acc=1.000000\n",
      "epoch=26 discriminator_loss=1.379756 real_acc=0.640000 fake_acc=0.660000\n",
      "epoch=27 discriminator_loss=1.381458 real_acc=0.650000 fake_acc=0.530000\n",
      "epoch=28 discriminator_loss=1.378778 real_acc=0.090000 fake_acc=0.950000\n",
      "epoch=29 discriminator_loss=1.377194 real_acc=0.630000 fake_acc=0.430000\n",
      "epoch=30 discriminator_loss=1.375420 real_acc=0.430000 fake_acc=0.690000\n",
      "epoch=31 discriminator_loss=1.372711 real_acc=0.450000 fake_acc=0.710000\n",
      "epoch=32 discriminator_loss=1.361908 real_acc=0.490000 fake_acc=0.670000\n",
      "epoch=33 discriminator_loss=1.366993 real_acc=0.680000 fake_acc=0.460000\n",
      "epoch=34 discriminator_loss=1.369682 real_acc=0.080000 fake_acc=0.940000\n",
      "epoch=35 discriminator_loss=1.401563 real_acc=1.000000 fake_acc=0.020000\n",
      "epoch=36 discriminator_loss=1.360682 real_acc=0.520000 fake_acc=0.640000\n",
      "epoch=37 discriminator_loss=1.373357 real_acc=0.020000 fake_acc=0.990000\n",
      "epoch=38 discriminator_loss=1.369045 real_acc=0.560000 fake_acc=0.500000\n",
      "epoch=39 discriminator_loss=1.369568 real_acc=0.970000 fake_acc=0.080000\n",
      "epoch=40 discriminator_loss=1.364272 real_acc=0.640000 fake_acc=0.440000\n",
      "epoch=41 discriminator_loss=1.350613 real_acc=0.190000 fake_acc=0.920000\n",
      "epoch=42 discriminator_loss=1.355880 real_acc=0.250000 fake_acc=0.850000\n",
      "epoch=43 discriminator_loss=1.386392 real_acc=0.790000 fake_acc=0.230000\n",
      "epoch=44 discriminator_loss=1.357367 real_acc=0.850000 fake_acc=0.220000\n",
      "epoch=45 discriminator_loss=1.366994 real_acc=0.600000 fake_acc=0.550000\n",
      "epoch=46 discriminator_loss=1.376698 real_acc=0.180000 fake_acc=0.890000\n",
      "epoch=47 discriminator_loss=1.365879 real_acc=0.470000 fake_acc=0.650000\n",
      "epoch=48 discriminator_loss=1.369800 real_acc=0.830000 fake_acc=0.160000\n",
      "epoch=49 discriminator_loss=1.377976 real_acc=0.770000 fake_acc=0.360000\n",
      "epoch=50 discriminator_loss=1.381563 real_acc=0.280000 fake_acc=0.700000\n",
      "epoch=51 discriminator_loss=1.375377 real_acc=0.370000 fake_acc=0.620000\n",
      "epoch=52 discriminator_loss=1.375119 real_acc=0.640000 fake_acc=0.470000\n",
      "epoch=53 discriminator_loss=1.367279 real_acc=0.700000 fake_acc=0.360000\n",
      "epoch=54 discriminator_loss=1.356449 real_acc=0.500000 fake_acc=0.610000\n",
      "epoch=55 discriminator_loss=1.347575 real_acc=0.480000 fake_acc=0.670000\n",
      "epoch=56 discriminator_loss=1.359153 real_acc=0.550000 fake_acc=0.580000\n",
      "epoch=57 discriminator_loss=1.353507 real_acc=0.680000 fake_acc=0.490000\n",
      "epoch=58 discriminator_loss=1.367267 real_acc=0.500000 fake_acc=0.620000\n",
      "epoch=59 discriminator_loss=1.329315 real_acc=0.650000 fake_acc=0.590000\n",
      "epoch=60 discriminator_loss=1.337203 real_acc=0.580000 fake_acc=0.690000\n",
      "epoch=61 discriminator_loss=1.367015 real_acc=0.490000 fake_acc=0.600000\n",
      "epoch=62 discriminator_loss=1.373151 real_acc=0.770000 fake_acc=0.400000\n",
      "epoch=63 discriminator_loss=1.376033 real_acc=0.440000 fake_acc=0.660000\n",
      "epoch=64 discriminator_loss=1.352895 real_acc=0.450000 fake_acc=0.660000\n",
      "epoch=65 discriminator_loss=1.338904 real_acc=0.690000 fake_acc=0.460000\n",
      "epoch=66 discriminator_loss=1.355718 real_acc=0.680000 fake_acc=0.480000\n",
      "epoch=67 discriminator_loss=1.390064 real_acc=0.340000 fake_acc=0.630000\n",
      "epoch=68 discriminator_loss=1.365119 real_acc=0.670000 fake_acc=0.370000\n",
      "epoch=69 discriminator_loss=1.339911 real_acc=0.600000 fake_acc=0.560000\n",
      "epoch=70 discriminator_loss=1.356308 real_acc=0.540000 fake_acc=0.650000\n",
      "epoch=71 discriminator_loss=1.352762 real_acc=0.550000 fake_acc=0.530000\n",
      "epoch=72 discriminator_loss=1.408362 real_acc=0.720000 fake_acc=0.240000\n",
      "epoch=73 discriminator_loss=1.358632 real_acc=0.350000 fake_acc=0.750000\n",
      "epoch=74 discriminator_loss=1.380112 real_acc=0.640000 fake_acc=0.420000\n",
      "epoch=75 discriminator_loss=1.341777 real_acc=0.620000 fake_acc=0.580000\n",
      "epoch=76 discriminator_loss=1.372428 real_acc=0.700000 fake_acc=0.490000\n",
      "epoch=77 discriminator_loss=1.376568 real_acc=0.300000 fake_acc=0.760000\n",
      "epoch=78 discriminator_loss=1.339603 real_acc=0.670000 fake_acc=0.530000\n",
      "epoch=79 discriminator_loss=1.364514 real_acc=0.660000 fake_acc=0.390000\n",
      "epoch=80 discriminator_loss=1.329882 real_acc=0.630000 fake_acc=0.640000\n",
      "epoch=81 discriminator_loss=1.366298 real_acc=0.390000 fake_acc=0.660000\n",
      "epoch=82 discriminator_loss=1.378717 real_acc=0.720000 fake_acc=0.300000\n",
      "epoch=83 discriminator_loss=1.371069 real_acc=0.700000 fake_acc=0.430000\n",
      "epoch=84 discriminator_loss=1.352318 real_acc=0.390000 fake_acc=0.730000\n",
      "epoch=85 discriminator_loss=1.342632 real_acc=0.740000 fake_acc=0.510000\n",
      "epoch=86 discriminator_loss=1.369031 real_acc=0.710000 fake_acc=0.360000\n",
      "epoch=87 discriminator_loss=1.349232 real_acc=0.300000 fake_acc=0.830000\n",
      "epoch=88 discriminator_loss=1.382731 real_acc=0.720000 fake_acc=0.350000\n",
      "epoch=89 discriminator_loss=1.336820 real_acc=0.740000 fake_acc=0.440000\n",
      "epoch=90 discriminator_loss=1.360017 real_acc=0.410000 fake_acc=0.680000\n",
      "epoch=91 discriminator_loss=1.361585 real_acc=0.600000 fake_acc=0.500000\n",
      "epoch=92 discriminator_loss=1.362822 real_acc=0.670000 fake_acc=0.460000\n",
      "epoch=93 discriminator_loss=1.339460 real_acc=0.510000 fake_acc=0.650000\n",
      "epoch=94 discriminator_loss=1.337410 real_acc=0.590000 fake_acc=0.610000\n",
      "epoch=95 discriminator_loss=1.334841 real_acc=0.800000 fake_acc=0.440000\n",
      "epoch=96 discriminator_loss=1.344798 real_acc=0.420000 fake_acc=0.830000\n",
      "epoch=97 discriminator_loss=1.372283 real_acc=0.630000 fake_acc=0.380000\n",
      "epoch=98 discriminator_loss=1.380721 real_acc=0.800000 fake_acc=0.270000\n",
      "epoch=99 discriminator_loss=1.391190 real_acc=0.080000 fake_acc=0.980000\n",
      "epoch=100 discriminator_loss=1.392671 real_acc=0.890000 fake_acc=0.110000\n",
      "epoch=101 discriminator_loss=1.389062 real_acc=0.670000 fake_acc=0.270000\n",
      "epoch=102 discriminator_loss=1.337129 real_acc=0.270000 fake_acc=0.860000\n",
      "epoch=103 discriminator_loss=1.351295 real_acc=0.520000 fake_acc=0.730000\n",
      "epoch=104 discriminator_loss=1.365970 real_acc=0.910000 fake_acc=0.250000\n",
      "epoch=105 discriminator_loss=1.348544 real_acc=0.540000 fake_acc=0.650000\n",
      "epoch=106 discriminator_loss=1.386842 real_acc=0.240000 fake_acc=0.810000\n",
      "epoch=107 discriminator_loss=1.342565 real_acc=0.820000 fake_acc=0.330000\n",
      "epoch=108 discriminator_loss=1.344061 real_acc=0.780000 fake_acc=0.290000\n",
      "epoch=109 discriminator_loss=1.332383 real_acc=0.290000 fake_acc=0.860000\n",
      "epoch=110 discriminator_loss=1.359168 real_acc=0.690000 fake_acc=0.400000\n",
      "epoch=111 discriminator_loss=1.341704 real_acc=0.760000 fake_acc=0.450000\n",
      "epoch=112 discriminator_loss=1.326774 real_acc=0.420000 fake_acc=0.720000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=113 discriminator_loss=1.336416 real_acc=0.550000 fake_acc=0.610000\n",
      "epoch=114 discriminator_loss=1.350960 real_acc=0.750000 fake_acc=0.410000\n",
      "epoch=115 discriminator_loss=1.324332 real_acc=0.690000 fake_acc=0.530000\n",
      "epoch=116 discriminator_loss=1.336168 real_acc=0.400000 fake_acc=0.760000\n",
      "epoch=117 discriminator_loss=1.351202 real_acc=0.720000 fake_acc=0.400000\n",
      "epoch=118 discriminator_loss=1.357288 real_acc=0.680000 fake_acc=0.510000\n",
      "epoch=119 discriminator_loss=1.364236 real_acc=0.340000 fake_acc=0.780000\n",
      "epoch=120 discriminator_loss=1.329124 real_acc=0.790000 fake_acc=0.480000\n",
      "epoch=121 discriminator_loss=1.331927 real_acc=0.590000 fake_acc=0.580000\n",
      "epoch=122 discriminator_loss=1.394864 real_acc=0.290000 fake_acc=0.740000\n",
      "epoch=123 discriminator_loss=1.369321 real_acc=0.840000 fake_acc=0.240000\n",
      "epoch=124 discriminator_loss=1.366453 real_acc=0.630000 fake_acc=0.470000\n",
      "epoch=125 discriminator_loss=1.390067 real_acc=0.150000 fake_acc=0.880000\n",
      "epoch=126 discriminator_loss=1.321805 real_acc=0.970000 fake_acc=0.220000\n",
      "epoch=127 discriminator_loss=1.328545 real_acc=0.730000 fake_acc=0.460000\n",
      "epoch=128 discriminator_loss=1.344983 real_acc=0.240000 fake_acc=0.900000\n",
      "epoch=129 discriminator_loss=1.332999 real_acc=0.680000 fake_acc=0.440000\n",
      "epoch=130 discriminator_loss=1.360269 real_acc=0.890000 fake_acc=0.230000\n",
      "epoch=131 discriminator_loss=1.313053 real_acc=0.580000 fake_acc=0.720000\n",
      "epoch=132 discriminator_loss=1.333807 real_acc=0.320000 fake_acc=0.830000\n",
      "epoch=133 discriminator_loss=1.345599 real_acc=0.690000 fake_acc=0.510000\n",
      "epoch=134 discriminator_loss=1.390281 real_acc=0.780000 fake_acc=0.260000\n",
      "epoch=135 discriminator_loss=1.362317 real_acc=0.450000 fake_acc=0.670000\n",
      "epoch=136 discriminator_loss=1.336230 real_acc=0.480000 fake_acc=0.740000\n",
      "epoch=137 discriminator_loss=1.309924 real_acc=0.680000 fake_acc=0.560000\n",
      "epoch=138 discriminator_loss=1.368952 real_acc=0.770000 fake_acc=0.370000\n",
      "epoch=139 discriminator_loss=1.325140 real_acc=0.570000 fake_acc=0.660000\n",
      "epoch=140 discriminator_loss=1.330627 real_acc=0.420000 fake_acc=0.760000\n",
      "epoch=141 discriminator_loss=1.417535 real_acc=0.700000 fake_acc=0.310000\n",
      "epoch=142 discriminator_loss=1.288809 real_acc=0.790000 fake_acc=0.490000\n",
      "epoch=143 discriminator_loss=1.370414 real_acc=0.460000 fake_acc=0.540000\n",
      "epoch=144 discriminator_loss=1.339244 real_acc=0.530000 fake_acc=0.600000\n",
      "epoch=145 discriminator_loss=1.362165 real_acc=0.760000 fake_acc=0.460000\n",
      "epoch=146 discriminator_loss=1.356860 real_acc=0.640000 fake_acc=0.560000\n",
      "epoch=147 discriminator_loss=1.339235 real_acc=0.350000 fake_acc=0.800000\n",
      "epoch=148 discriminator_loss=1.385678 real_acc=0.800000 fake_acc=0.310000\n",
      "epoch=149 discriminator_loss=1.318246 real_acc=0.770000 fake_acc=0.420000\n",
      "epoch=150 discriminator_loss=1.342883 real_acc=0.240000 fake_acc=0.920000\n",
      "epoch=151 discriminator_loss=1.360268 real_acc=0.600000 fake_acc=0.520000\n",
      "epoch=152 discriminator_loss=1.343843 real_acc=0.900000 fake_acc=0.270000\n",
      "epoch=153 discriminator_loss=1.360409 real_acc=0.430000 fake_acc=0.600000\n",
      "epoch=154 discriminator_loss=1.360540 real_acc=0.350000 fake_acc=0.790000\n",
      "epoch=155 discriminator_loss=1.332955 real_acc=0.900000 fake_acc=0.320000\n",
      "epoch=156 discriminator_loss=1.358869 real_acc=0.590000 fake_acc=0.470000\n",
      "epoch=157 discriminator_loss=1.350756 real_acc=0.380000 fake_acc=0.810000\n",
      "epoch=158 discriminator_loss=1.344420 real_acc=0.640000 fake_acc=0.590000\n",
      "epoch=159 discriminator_loss=1.342809 real_acc=0.830000 fake_acc=0.350000\n",
      "epoch=160 discriminator_loss=1.366235 real_acc=0.370000 fake_acc=0.660000\n",
      "epoch=161 discriminator_loss=1.355925 real_acc=0.330000 fake_acc=0.740000\n",
      "epoch=162 discriminator_loss=1.340879 real_acc=0.820000 fake_acc=0.290000\n",
      "epoch=163 discriminator_loss=1.347461 real_acc=0.740000 fake_acc=0.300000\n",
      "epoch=164 discriminator_loss=1.353289 real_acc=0.380000 fake_acc=0.760000\n",
      "epoch=165 discriminator_loss=1.376603 real_acc=0.490000 fake_acc=0.580000\n",
      "epoch=166 discriminator_loss=1.355794 real_acc=0.740000 fake_acc=0.370000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ff8ce7b54487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreal_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfake_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c4cf1f86e291>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epoches)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mreal_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c4cf1f86e291>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(discriminator)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# both fake and real seq have shape [Batch_size, seq_len, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mfake_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen_gaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         real_seq = tf.reshape(tf.stack([tf.reshape(gen_gaps_real(seq_len), (1, seq_len))\\\n\u001b[1;32m      8\u001b[0m                                       for _ in range(batch_size)]),[batch_size,seq_len,1])\n",
      "\u001b[0;32m<ipython-input-2-c4cf1f86e291>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# both fake and real seq have shape [Batch_size, seq_len, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mfake_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen_gaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         real_seq = tf.reshape(tf.stack([tf.reshape(gen_gaps_real(seq_len), (1, seq_len))\\\n\u001b[1;32m      8\u001b[0m                                       for _ in range(batch_size)]),[batch_size,seq_len,1])\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_loss = []\n",
    "real_acc = []\n",
    "fake_acc = []\n",
    "training(EPOCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "Ud8R_l_Gae02",
    "outputId": "cbfbfcf8-e5e9-4ca9-ae9a-0cda58f96f0c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B = 20\n",
    "n = 50\n",
    "W = np.hamming(B)\n",
    "W /= W.sum()\n",
    "plt.figure(figsize = (21,8))\n",
    "# plt.plot(range(len(real_acc)),np.convolve(real_acc, W, mode='same'),label='real')\n",
    "# plt.plot(range(len(real_acc)),np.convolve(fake_acc, W, mode='same'),label='fake')\n",
    "plt.plot(range(len(real_acc)-n+1), moving_average(real_acc,n),label='real')\n",
    "plt.plot(range(len(fake_acc)-n+1), moving_average(fake_acc,n),label='fake')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "train_D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
