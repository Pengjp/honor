{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rEP8Ti9V-1B4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, layers, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU,Conv1D,Dropout,MaxPooling1D,SimpleRNN,LocallyConnected1D\n",
    "from tensorflow.keras.layers import Flatten,ReLU, BatchNormalization,AveragePooling1D, LSTM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def runif():\n",
    "    return tf.random.uniform([1], dtype=tf.float64)[0]\n",
    "    # return tf.constant(.8, tf.float32)\n",
    "\n",
    "def rexp():\n",
    "    return -tf.math.log(runif())\n",
    "\n",
    "\n",
    "def exprelu(x):\n",
    "    return tf.where(x > 0, tf.math.expm1(x), tf.zeros_like(x))\n",
    "\n",
    "def reloid(x):\n",
    "    \"(sigma(x[1]), ..., sigma(x[-2]), relu(x[-1])\"\n",
    "    return tf.concat([tf.nn.sigmoid(x[:-1]), tf.math.exp(x[-1:])], axis=0)\n",
    "\n",
    "def reloid_derivative(x):\n",
    "    return tf.concat(\n",
    "        [\n",
    "            tf.nn.sigmoid(x[:-1])\n",
    "            * (1 - tf.nn.sigmoid(x[:-1])),  # derivative of sigmoid\n",
    "            tf.math.exp(x[-1:]),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "def S(x, w, v, b):\n",
    "    \"\"\"\n",
    "    x: scalar\n",
    "    w, v, b: (3, H)\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x, dtype=\"float64\")\n",
    "    # tf.debugging.assert_positive(x, message=\"R: x>0\")\n",
    "    exp_w_v = tf.math.exp([w, v])\n",
    "    ew = exp_w_v[0]\n",
    "    ev = exp_w_v[1]\n",
    "    # b = tf.math.sigmoid(b) # try this  # JT - bug. was sigb\n",
    "    ew = tf.concat([ew[:-1], tf.ones_like(ew[-1:]),], axis=0,)\n",
    "    x = tf.reshape(x, (1, 1))\n",
    "    return tf.transpose(ev) @ reloid(ew @ x + b)\n",
    "\n",
    "@tf.function\n",
    "def R(x, w, v, b):\n",
    "    return S(tf.math.log(x), w, v, b)\n",
    "\n",
    "@tf.function\n",
    "def Rinv(y, w, v, b):\n",
    "    y = tf.convert_to_tensor(y, dtype=\"float64\")\n",
    "    # y = tf.reshape(y, (-1,))[0]\n",
    "    # as x -> oo, R is asymyptotic to exp(v[-1] + w[-1]) x\n",
    "    # fixme: calculate this exactly.\n",
    "    x_left = tf.convert_to_tensor([[0.0]], tf.float64)\n",
    "    x_right = tf.convert_to_tensor([[1e8]], tf.float64)\n",
    "    # tf.print((x_left, x_right))\n",
    "    # tf.print(\"y\", y)\n",
    "    # tf.print('y',y)\n",
    "    # tf.debugging.assert_greater(R(x_right, w, v, b), y, message=\"R(x_right)>y inv\")\n",
    "\n",
    "    def cond(xl, xr):\n",
    "        # tf.print(xl, xr)\n",
    "        xi = (xl + xr) / 2.0\n",
    "        yi = R(xi, w, v, b)[0, 0]\n",
    "        return abs(y - yi) > 1e-6\n",
    "\n",
    "    def body(xl, xr):\n",
    "        xi = (xl + xr) / 2.0\n",
    "        yi = R(xi, w, v, b)[0, 0]\n",
    "        left = tf.cast(yi < y, dtype=\"float64\")\n",
    "        xl = left * xi + (1.0 - left) * xl\n",
    "        xr = (1.0 - left) * xi + left * xr\n",
    "        return (xl, xr)\n",
    "        # print(y, x_i, y_i)\n",
    "\n",
    "    xl, xr = tf.while_loop(cond, body, (x_left, x_right))\n",
    "    return (xl + xr) / 2.0\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_Rinv(y, w, v, b):\n",
    "    y = tf.convert_to_tensor(y, dtype=\"float64\")\n",
    "    x = Rinv(y, w, v, b)\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch([x, w, v, b])\n",
    "        y = R(x, w, v, b)\n",
    "    dR_dw, dR_dv, dR_db, dR_dx = g.gradient(y, [w, v, b, x])\n",
    "\n",
    "    def grad(dx):\n",
    "        return dx / dR_dx, -dx * dR_dw / dR_dx, -dx * dR_dv / dR_dx, -dx * dR_db / dR_dx\n",
    "\n",
    "    return x, grad\n",
    "\n",
    "mu = 1e-4\n",
    "rho = 1e-5\n",
    "\n",
    "def _gen_gaps(k: int, _R, _Rinv,) -> tf.Tensor:\n",
    "    \"\"\"Return k gaps sampled from genetic distribution with rate function eta.\"\"\"\n",
    "    z = tf.convert_to_tensor([[rexp()]])\n",
    "    x = _Rinv(z)  # initialize x by sampling from prior\n",
    "    tf.debugging.assert_positive(x, message=\"gen_gaps first x\")\n",
    "\n",
    "    gap = tf.constant([[0.0]], dtype=tf.float64)\n",
    "    j = 0\n",
    "    ta = tf.TensorArray(tf.float64, size=k + 1)\n",
    "\n",
    "    while tf.less(j, k + 1):\n",
    "        # x' satisfies R(x') - R(u*x) = Z => x' = Rinv(Z + R(u*x))\n",
    "        u = runif()\n",
    "        z = rexp()\n",
    "        u_x = tf.convert_to_tensor([[u * x]])\n",
    "        r_u_x = _R(u_x)  # compute R(u_x)\n",
    "        x = _Rinv(z + r_u_x)  # segment height\n",
    "        # tf.print(x)\n",
    "        # tf.print(z+r_u_x,\"\\n\")\n",
    "        with tf.control_dependencies(\n",
    "            [\n",
    "                tf.debugging.assert_all_finite(x, \"second x\"),\n",
    "                tf.debugging.assert_positive(x, message=\"gen_gaps second x\"),\n",
    "            ]\n",
    "        ):\n",
    "            next_event = rexp() / (x * (mu + rho))\n",
    "            tf.debugging.assert_positive(next_event, message=\"gen_gaps first gap\")\n",
    "            gap += next_event  # length to next event\n",
    "        while runif() < (mu / (mu + rho)) and tf.less(j, k + 1):\n",
    "            ta = ta.write(j, gap)\n",
    "            gap *= 0.0\n",
    "            j += 1\n",
    "            next_event = rexp() / (x * (mu + rho))\n",
    "            tf.debugging.assert_positive(next_event, message=\"gen_gaps second gap\")\n",
    "            gap += next_event  # length to next event\n",
    "\n",
    "    gaps = ta.stack()[1:]  # first obs suffers from inspection paradox?\n",
    "    with tf.control_dependencies(\n",
    "        [\n",
    "            tf.debugging.assert_positive(\n",
    "                gaps, message=\"gaps have non-positive entry\", summarize=100\n",
    "            )\n",
    "        ]\n",
    "    ):\n",
    "        return gaps\n",
    "\n",
    "\n",
    "def R_learned(x, generator):\n",
    "    return R(x, generator.weights[0], generator.weights[1], generator.weights[2])\n",
    "\n",
    "thresh = tf.constant([1e-1], dtype=\"float64\", shape=(1,))\n",
    "# eta = TfPoly(x=[0., 0.1, np.inf], c=[[1/100, 1.]])\n",
    "def eta(x):\n",
    "    x = tf.cast(x, \"float64\")\n",
    "    one = tf.ones(shape=[1,], dtype=\"float64\",)\n",
    "    return tf.cast(tf.where(x < thresh, 1 / 100, one), \"float64\")\n",
    "\n",
    "def R_real(x):\n",
    "    \"\"\"R_real(x) = integral_0^x eta(t) dt\"\"\"\n",
    "    x = tf.cast(x, \"float64\")\n",
    "    x = tf.reshape(x, (1, tf.size(x)))\n",
    "    tf.debugging.assert_positive(x, message=\"R_real: x>0\")\n",
    "    return tf.cast(\n",
    "        tf.where(x < thresh, x / 100.0, thresh / 100.0 + (x - thresh)), \"float64\"\n",
    "    )\n",
    "    # return x\n",
    "\n",
    "\n",
    "def R_real_inv(y):\n",
    "    y = tf.cast(y, \"float64\")\n",
    "    tf.debugging.assert_positive(y, message=\"R_real: x>0\")\n",
    "    return tf.cast(\n",
    "        tf.where(y < thresh / 100.0, y * 100.0, y - (thresh / 100.0 - thresh)),\n",
    "        \"float64\",\n",
    "    )\n",
    "\n",
    "@tf.function\n",
    "def gen_gaps(\n",
    "    w, v, b, k,\n",
    "):\n",
    "    R_ = lambda x: R(x, w, v, b)\n",
    "    Rinv_ = lambda z: custom_Rinv(z, w, v, b)\n",
    "    return _gen_gaps(k, R_, Rinv_)\n",
    "\n",
    "@tf.function\n",
    "def gen_gaps_real(k: int,):\n",
    "    return _gen_gaps(k, R_real, R_real_inv,)\n",
    "\n",
    "def discriminator_objective(d_x, g_z):\n",
    "    \"\"\"\n",
    "    d_x = real output\n",
    "    g_z = fake output\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(\n",
    "        tf.ones_like(d_x), d_x\n",
    "    )  # If we feed the discriminator with real images, we assume they all are the right pictures --> Because of that label == 1\n",
    "    fake_loss = cross_entropy(\n",
    "        tf.zeros_like(g_z), g_z\n",
    "    )  # Each noise we feed in are fakes image --> Because of that labels are 0\n",
    "    total_loss = real_loss + fake_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ke3db-pVL6Sj"
   },
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy()\n",
    "# @tf.function\n",
    "def training_step(discriminator):\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # both fake and real seq have shape [Batch_size, seq_len, 1]\n",
    "        fake_seq = tf.reshape(tf.stack([gen_gaps(w, v, b, seq_len) for _ in range(batch_size)]),[batch_size,seq_len,1])\n",
    "        real_seq = tf.reshape(tf.stack([tf.reshape(gen_gaps_real(seq_len), (1, seq_len))\\\n",
    "                                      for _ in range(batch_size)]),[batch_size,seq_len,1])\n",
    "        d_x_true = discriminator(tf.expand_dims(tf.math.log(real_seq),-1))\n",
    "        d_x_fake = discriminator(tf.expand_dims(tf.math.log(fake_seq),-1))\n",
    "        \n",
    "        discriminator_loss = discriminator_objective(d_x_true, d_x_fake)\n",
    "        # Adjusting Gradient of Discriminator\n",
    "        gradients_of_discriminator = disc_tape.gradient(\n",
    "            discriminator_loss, discriminator.trainable_variables\n",
    "        )\n",
    "        discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "        )  # Takes a list of gradient and variables pairs\n",
    "    m.reset_states()\n",
    "    m.update_state(tf.ones_like(d_x_true),tf.math.sigmoid(d_x_true))\n",
    "    real_acc = m.result()\n",
    "\n",
    "    m.reset_states()\n",
    "    m.update_state(tf.zeros_like(d_x_fake),tf.math.sigmoid(d_x_fake))\n",
    "    fake_acc = m.result()\n",
    "\n",
    "    return discriminator_loss, real_acc, fake_acc\n",
    "  \n",
    "def training(epoches):\n",
    "    for epoch in range(epoches + 1):\n",
    "        disc_loss,real,fake = training_step(discriminator)\n",
    "        d_loss.append(disc_loss)\n",
    "        real_acc.append(real)\n",
    "        fake_acc.append(fake)\n",
    "        print(\"epoch=%d discriminator_loss=%f real_acc=%f fake_acc=%f\"\n",
    "                % (epoch, disc_loss, real, fake))\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S9jhuPJK-703"
   },
   "outputs": [],
   "source": [
    "seq_len = 1500\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "EPOCHES = 500\n",
    "\n",
    "# step = tf.Variable(0, trainable=False)\n",
    "# boundaries = [100, 200]\n",
    "# values = [0.01, 0.005, 0.001]\n",
    "# learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "#     boundaries, values)\n",
    "\n",
    "# Later, whenever we perform an optimization step, we pass in the step.\n",
    "# learning_rate = learning_rate_fn(step)\n",
    "\n",
    "discriminator_optimizer = optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ec5wj8J8NNHq"
   },
   "outputs": [],
   "source": [
    "#upload weights.csv to colab\n",
    "w,v,b=np.loadtxt('weights.csv',delimiter=',')\n",
    "w = tf.expand_dims(w,-1)\n",
    "v = tf.expand_dims(v,-1)\n",
    "b = tf.expand_dims(b,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "NWDNT8MzOec_",
    "outputId": "6c1e1581-2078-4046-b458-927f33c89271"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = tf.math.log(gen_gaps(w, v, b, 1000))\n",
    "y = tf.math.log(gen_gaps_real(1000))\n",
    "plt.hist(x.numpy().reshape(-1),label = 'fake')\n",
    "plt.hist(y.numpy().reshape(-1), alpha=0.4,label='real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = gen_gaps(w, v, b, 1000)\n",
    "y = gen_gaps_real(1000)\n",
    "plt.hist(x.numpy().reshape(-1),label = 'fake')\n",
    "plt.hist(y.numpy().reshape(-1), alpha=0.4,label='real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "LN8jwzyXLxoW",
    "outputId": "d0b2b7c7-d9f1-42f8-d287-842ec8fce0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "locally_connected1d (Locally (None, 1497, 512)         3832320   \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 1497, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1497, 512)         2048      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1497, 500)         2026000   \n",
      "_________________________________________________________________\n",
      "locally_connected1d_1 (Local (None, 1494, 256)         765310464 \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 1494, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1494, 256)         1024      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1494, 250)         507000    \n",
      "_________________________________________________________________\n",
      "locally_connected1d_2 (Local (None, 1491, 128)         191038848 \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 1491, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1491, 128)         512       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1491, 125)         127000    \n",
      "_________________________________________________________________\n",
      "locally_connected1d_3 (Local (None, 1488, 64)          47711232  \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 1488, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1488, 64)          256       \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1488, 60)          30000     \n",
      "_________________________________________________________________\n",
      "locally_connected1d_4 (Local (None, 1485, 32)          11452320  \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 1485, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1485, 32)          128       \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30)                7560      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 1,022,046,743\n",
      "Trainable params: 1,022,044,759\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential(\n",
    "    [\n",
    "     Input(shape=(seq_len,1)),\n",
    "     LocallyConnected1D(512, 4),\n",
    "     ReLU(),\n",
    "     BatchNormalization(),\n",
    "     LSTM(500, return_sequences=True),\n",
    "        \n",
    "\n",
    "     LocallyConnected1D(256, 4),\n",
    "     ReLU(),\n",
    "     BatchNormalization(),\n",
    "     LSTM(250, return_sequences=True),\n",
    "        \n",
    "     LocallyConnected1D(128, 4),\n",
    "     ReLU(),\n",
    "     BatchNormalization(),\n",
    "     LSTM(125, return_sequences=True),\n",
    "        \n",
    "     LocallyConnected1D(64, 4),\n",
    "     ReLU(),\n",
    "     BatchNormalization(),\n",
    "     LSTM(60, return_sequences=True),\n",
    "                \n",
    "     LocallyConnected1D(32, 4),\n",
    "     ReLU(),\n",
    "     BatchNormalization(),\n",
    "     LSTM(30),\n",
    "        \n",
    "#      LSTM(25),\n",
    "#      LocallyConnected1D(28, 4),\n",
    "#      ReLU(),\n",
    "#      BatchNormalization(),\n",
    "#      LocallyConnected1D(128, 4),\n",
    "#      ReLU(),\n",
    "#      MaxPooling1D(),\n",
    "        \n",
    "#      LocallyConnected1D(64, 4),\n",
    "#      ReLU(),\n",
    "#      MaxPooling1D(),\n",
    "    \n",
    "#      LocallyConnected1D(32, 4),\n",
    "#      ReLU(),\n",
    "#      MaxPooling1D(),\n",
    "\n",
    "#      LocallyConnected1D(16, 4),\n",
    "#      MaxPooling1D(),\n",
    "#      ReLU(),\n",
    "        \n",
    "#      LocallyConnected1D(8, 4),\n",
    "#      MaxPooling1D(),\n",
    "#      ReLU(),\n",
    "#      AveragePooling1D(),\n",
    "#      Dense(30,'relu'),\n",
    "#      Dense(10,'relu'),\n",
    "#      Flatten(),\n",
    "#      Dense(50),\n",
    "#      ReLU(),\n",
    "#      Dense(30),\n",
    "#      ReLU(),\n",
    "     Dense(1)\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3XQB77QPNFVt",
    "outputId": "ea604227-c9bc-4685-8265-ad60c7e63bcd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=1 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=2 discriminator_loss=1.386513 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=3 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=4 discriminator_loss=1.386374 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=5 discriminator_loss=1.386366 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=6 discriminator_loss=1.386298 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=7 discriminator_loss=1.386341 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=8 discriminator_loss=1.386337 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=9 discriminator_loss=1.386297 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=10 discriminator_loss=1.386302 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=11 discriminator_loss=1.386318 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=12 discriminator_loss=1.386316 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=13 discriminator_loss=1.386301 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=14 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=15 discriminator_loss=1.386301 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=16 discriminator_loss=1.386308 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=17 discriminator_loss=1.386304 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=18 discriminator_loss=1.386297 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=19 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=20 discriminator_loss=1.386298 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=21 discriminator_loss=1.386301 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=22 discriminator_loss=1.386300 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=23 discriminator_loss=1.386297 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=24 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=25 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=26 discriminator_loss=1.386297 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=27 discriminator_loss=1.386298 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=28 discriminator_loss=1.386297 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=29 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=30 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=31 discriminator_loss=1.386295 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=32 discriminator_loss=1.386296 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=33 discriminator_loss=1.386296 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=34 discriminator_loss=1.386295 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=35 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=36 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=37 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=38 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=39 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=40 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=41 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=42 discriminator_loss=1.386295 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=43 discriminator_loss=1.386295 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=44 discriminator_loss=1.386295 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=45 discriminator_loss=1.386295 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=46 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=47 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=48 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=49 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=50 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=51 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=52 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=53 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=54 discriminator_loss=1.386295 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=55 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=56 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=57 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=58 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=59 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=60 discriminator_loss=1.386295 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=61 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=62 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=63 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=64 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=65 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=66 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=67 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=68 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=69 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=70 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=71 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=72 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=73 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=74 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=75 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=76 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=77 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=78 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=79 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=80 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=81 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=82 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=83 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=84 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=85 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=86 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=87 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=88 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=89 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=90 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=91 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=92 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=93 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=94 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=95 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=96 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=97 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=98 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=99 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=100 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=101 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=102 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=103 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=104 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=105 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=106 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=107 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=108 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=109 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=110 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=111 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=112 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=113 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=114 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=115 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=116 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=117 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=118 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=119 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=120 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=121 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=122 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=123 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=124 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=125 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=126 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=127 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=128 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=129 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=130 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=131 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=132 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=133 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=134 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=135 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=136 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=137 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=138 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=139 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=140 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=141 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=142 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=143 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=144 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=145 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=146 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=147 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=148 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=149 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=150 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=151 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=152 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=153 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=154 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=155 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=156 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=157 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=158 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=159 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=160 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=161 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=162 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=163 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=164 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=165 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=166 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=167 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=168 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=169 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=170 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=171 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=172 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=173 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=174 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=175 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=176 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=177 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=178 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=179 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=180 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=181 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=182 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=183 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=184 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=185 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=186 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=187 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=188 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=189 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=190 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=191 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=192 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=193 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=194 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=195 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=196 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=197 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=198 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=199 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=200 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=201 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=202 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=203 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=204 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=205 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=206 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=207 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=208 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=209 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=210 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=211 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=212 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=213 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=214 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=215 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=216 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=217 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=218 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=219 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=220 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=221 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=222 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=223 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=224 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=225 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=226 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=227 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=228 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=229 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=230 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=231 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=232 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=233 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=234 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=235 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=236 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=237 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=238 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=239 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=240 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=241 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=242 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=243 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=244 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=245 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=246 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=247 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=248 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=249 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=250 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=251 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=252 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=253 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=254 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=255 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=256 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=257 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=258 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=259 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=260 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=261 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=262 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=263 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=264 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=265 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=266 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=267 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=268 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=269 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=270 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=271 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=272 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=273 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=274 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=275 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=276 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=277 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=278 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=279 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=280 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=281 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=282 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=283 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=284 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=285 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=286 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=287 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=288 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=289 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=290 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=291 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=292 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=293 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=294 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=295 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=296 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=297 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=298 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=299 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=300 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=301 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=302 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=303 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=304 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=305 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=306 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=307 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=308 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=309 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=310 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=311 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=312 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=313 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=314 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=315 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=316 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=317 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=318 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=319 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=320 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=321 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=322 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=323 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=324 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=325 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=326 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=327 discriminator_loss=1.386294 real_acc=0.359375 fake_acc=0.625000\n",
      "epoch=328 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=329 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=330 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=331 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=332 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=333 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=334 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=335 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=336 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=337 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=338 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=339 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=340 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=341 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=342 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=343 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=344 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=345 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=346 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=347 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=348 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=349 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=350 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=351 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=352 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=353 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=354 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=355 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=356 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=357 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=358 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=359 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=360 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=361 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=362 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=363 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=364 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=365 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=366 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=367 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=368 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=369 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=370 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=371 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=372 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=373 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=374 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=375 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=376 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=377 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=378 discriminator_loss=1.386294 real_acc=1.000000 fake_acc=0.000000\n",
      "epoch=379 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=380 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=381 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=382 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=383 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=384 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=385 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=386 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=387 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=388 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=389 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=390 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=391 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=392 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=393 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=394 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=395 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=396 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=397 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=398 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=399 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=400 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=401 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=402 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=403 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=404 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=405 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=406 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=407 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=408 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=409 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=410 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=411 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=412 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n",
      "epoch=413 discriminator_loss=1.386294 real_acc=0.000000 fake_acc=1.000000\n"
     ]
    }
   ],
   "source": [
    "d_loss = []\n",
    "real_acc = []\n",
    "fake_acc = []\n",
    "training(EPOCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (21,8))\n",
    "plt.plot(range(len(real_acc)), (np.array(real_acc)+np.array(fake_acc))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "Ud8R_l_Gae02",
    "outputId": "cbfbfcf8-e5e9-4ca9-ae9a-0cda58f96f0c"
   },
   "outputs": [],
   "source": [
    "B = 20\n",
    "W = np.hamming(B)\n",
    "W /= W.sum()\n",
    "plt.figure(figsize = (21,8))\n",
    "plt.plot(range(len(real_acc)),np.convolve(real_acc, W, mode='same'),label='real')\n",
    "plt.plot(range(len(real_acc)),np.convolve(fake_acc, W, mode='same'),label='fake')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.applications.MobileNetV2().summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "train_D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
