{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rEP8Ti9V-1B4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, layers, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU,Conv1D,Dropout,MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten,ReLU, BatchNormalization,GlobalAveragePooling1D\n",
    "import numpy as np\n",
    "\n",
    "def runif():\n",
    "    return tf.random.uniform([1], dtype=tf.float64)[0]\n",
    "    # return tf.constant(.8, tf.float32)\n",
    "\n",
    "def rexp():\n",
    "    return -tf.math.log(runif())\n",
    "\n",
    "\n",
    "def exprelu(x):\n",
    "    return tf.where(x > 0, tf.math.expm1(x), tf.zeros_like(x))\n",
    "\n",
    "def reloid(x):\n",
    "    \"(sigma(x[1]), ..., sigma(x[-2]), relu(x[-1])\"\n",
    "    return tf.concat([tf.nn.sigmoid(x[:-1]), tf.math.exp(x[-1:])], axis=0)\n",
    "\n",
    "def reloid_derivative(x):\n",
    "    return tf.concat(\n",
    "        [\n",
    "            tf.nn.sigmoid(x[:-1])\n",
    "            * (1 - tf.nn.sigmoid(x[:-1])),  # derivative of sigmoid\n",
    "            tf.math.exp(x[-1:]),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "def S(x, w, v, b):\n",
    "    \"\"\"\n",
    "    x: scalar\n",
    "    w, v, b: (3, H)\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x, dtype=\"float64\")\n",
    "    # tf.debugging.assert_positive(x, message=\"R: x>0\")\n",
    "    exp_w_v = tf.math.exp([w, v])\n",
    "    ew = exp_w_v[0]\n",
    "    ev = exp_w_v[1]\n",
    "    # b = tf.math.sigmoid(b) # try this  # JT - bug. was sigb\n",
    "    ew = tf.concat([ew[:-1], tf.ones_like(ew[-1:]),], axis=0,)\n",
    "    x = tf.reshape(x, (1, 1))\n",
    "    return tf.transpose(ev) @ reloid(ew @ x + b)\n",
    "\n",
    "@tf.function\n",
    "def R(x, w, v, b):\n",
    "    return S(tf.math.log(x), w, v, b)\n",
    "\n",
    "@tf.function\n",
    "def Rinv(y, w, v, b):\n",
    "    y = tf.convert_to_tensor(y, dtype=\"float64\")\n",
    "    # y = tf.reshape(y, (-1,))[0]\n",
    "    # as x -> oo, R is asymyptotic to exp(v[-1] + w[-1]) x\n",
    "    # fixme: calculate this exactly.\n",
    "    x_left = tf.convert_to_tensor([[0.0]], tf.float64)\n",
    "    x_right = tf.convert_to_tensor([[1e8]], tf.float64)\n",
    "    # tf.print((x_left, x_right))\n",
    "    # tf.print(\"y\", y)\n",
    "    # tf.print('y',y)\n",
    "    # tf.debugging.assert_greater(R(x_right, w, v, b), y, message=\"R(x_right)>y inv\")\n",
    "\n",
    "    def cond(xl, xr):\n",
    "        # tf.print(xl, xr)\n",
    "        xi = (xl + xr) / 2.0\n",
    "        yi = R(xi, w, v, b)[0, 0]\n",
    "        return abs(y - yi) > 1e-6\n",
    "\n",
    "    def body(xl, xr):\n",
    "        xi = (xl + xr) / 2.0\n",
    "        yi = R(xi, w, v, b)[0, 0]\n",
    "        left = tf.cast(yi < y, dtype=\"float64\")\n",
    "        xl = left * xi + (1.0 - left) * xl\n",
    "        xr = (1.0 - left) * xi + left * xr\n",
    "        return (xl, xr)\n",
    "        # print(y, x_i, y_i)\n",
    "\n",
    "    xl, xr = tf.while_loop(cond, body, (x_left, x_right))\n",
    "    return (xl + xr) / 2.0\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_Rinv(y, w, v, b):\n",
    "    y = tf.convert_to_tensor(y, dtype=\"float64\")\n",
    "    x = Rinv(y, w, v, b)\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch([x, w, v, b])\n",
    "        y = R(x, w, v, b)\n",
    "    dR_dw, dR_dv, dR_db, dR_dx = g.gradient(y, [w, v, b, x])\n",
    "\n",
    "    def grad(dx):\n",
    "        return dx / dR_dx, -dx * dR_dw / dR_dx, -dx * dR_dv / dR_dx, -dx * dR_db / dR_dx\n",
    "\n",
    "    return x, grad\n",
    "\n",
    "mu = 1e-4\n",
    "rho = 1e-5\n",
    "\n",
    "def _gen_gaps(k: int, _R, _Rinv,) -> tf.Tensor:\n",
    "    \"\"\"Return k gaps sampled from genetic distribution with rate function eta.\"\"\"\n",
    "    z = tf.convert_to_tensor([[rexp()]])\n",
    "    x = _Rinv(z)  # initialize x by sampling from prior\n",
    "    tf.debugging.assert_positive(x, message=\"gen_gaps first x\")\n",
    "\n",
    "    gap = tf.constant([[0.0]], dtype=tf.float64)\n",
    "    j = 0\n",
    "    ta = tf.TensorArray(tf.float64, size=k + 1)\n",
    "\n",
    "    while tf.less(j, k + 1):\n",
    "        # x' satisfies R(x') - R(u*x) = Z => x' = Rinv(Z + R(u*x))\n",
    "        u = runif()\n",
    "        z = rexp()\n",
    "        u_x = tf.convert_to_tensor([[u * x]])\n",
    "        r_u_x = _R(u_x)  # compute R(u_x)\n",
    "        x = _Rinv(z + r_u_x)  # segment height\n",
    "        # tf.print(x)\n",
    "        # tf.print(z+r_u_x,\"\\n\")\n",
    "        with tf.control_dependencies(\n",
    "            [\n",
    "                tf.debugging.assert_all_finite(x, \"second x\"),\n",
    "                tf.debugging.assert_positive(x, message=\"gen_gaps second x\"),\n",
    "            ]\n",
    "        ):\n",
    "            next_event = rexp() / (x * (mu + rho))\n",
    "            tf.debugging.assert_positive(next_event, message=\"gen_gaps first gap\")\n",
    "            gap += next_event  # length to next event\n",
    "        while runif() < (mu / (mu + rho)) and tf.less(j, k + 1):\n",
    "            ta = ta.write(j, gap)\n",
    "            gap *= 0.0\n",
    "            j += 1\n",
    "            next_event = rexp() / (x * (mu + rho))\n",
    "            tf.debugging.assert_positive(next_event, message=\"gen_gaps second gap\")\n",
    "            gap += next_event  # length to next event\n",
    "\n",
    "    gaps = ta.stack()[1:]  # first obs suffers from inspection paradox?\n",
    "    with tf.control_dependencies(\n",
    "        [\n",
    "            tf.debugging.assert_positive(\n",
    "                gaps, message=\"gaps have non-positive entry\", summarize=100\n",
    "            )\n",
    "        ]\n",
    "    ):\n",
    "        return gaps\n",
    "\n",
    "\n",
    "def R_learned(x, generator):\n",
    "    return R(x, generator.weights[0], generator.weights[1], generator.weights[2])\n",
    "\n",
    "thresh = tf.constant([1e-1], dtype=\"float64\", shape=(1,))\n",
    "\n",
    "def eta(x):\n",
    "    x = tf.cast(x, \"float64\")\n",
    "    one = tf.ones(shape=[1,], dtype=\"float64\",)\n",
    "    return tf.cast(tf.where(x < thresh, 1 / 100, one), \"float64\")\n",
    "\n",
    "def R_real(x):\n",
    "    \"\"\"R_real(x) = integral_0^x eta(t) dt\"\"\"\n",
    "    x = tf.cast(x, \"float64\")\n",
    "    x = tf.reshape(x, (1, tf.size(x)))\n",
    "    tf.debugging.assert_positive(x, message=\"R_real: x>0\")\n",
    "    return tf.cast(\n",
    "        tf.where(x < thresh, x / 100.0, thresh / 100.0 + (x - thresh)), \"float64\"\n",
    "    )\n",
    "    # return x\n",
    "\n",
    "\n",
    "def R_real_inv(y):\n",
    "    y = tf.cast(y, \"float64\")\n",
    "    tf.debugging.assert_positive(y, message=\"R_real: x>0\")\n",
    "    return tf.cast(\n",
    "        tf.where(y < thresh / 100.0, y * 100.0, y - (thresh / 100.0 - thresh)),\n",
    "        \"float64\",\n",
    "    )\n",
    "\n",
    "@tf.function\n",
    "def gen_gaps(\n",
    "    w, v, b, k,\n",
    "):\n",
    "    R_ = lambda x: R(x, w, v, b)\n",
    "    Rinv_ = lambda z: custom_Rinv(z, w, v, b)\n",
    "    return _gen_gaps(k, R_, Rinv_)\n",
    "\n",
    "@tf.function\n",
    "def gen_gaps_real(k: int,):\n",
    "    return _gen_gaps(k, R_real, R_real_inv,)\n",
    "\n",
    "def discriminator_objective(d_x, g_z):\n",
    "    \"\"\"\n",
    "    d_x = real output\n",
    "    g_z = fake output\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(\n",
    "        tf.ones_like(d_x), d_x\n",
    "    )  # If we feed the discriminator with real images, we assume they all are the right pictures --> Because of that label == 1\n",
    "    fake_loss = cross_entropy(\n",
    "        tf.zeros_like(g_z), g_z\n",
    "    )  # Each noise we feed in are fakes image --> Because of that labels are 0\n",
    "    total_loss = real_loss + fake_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ke3db-pVL6Sj"
   },
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy()\n",
    "@tf.function\n",
    "def training_step(discriminator):\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        fake_seq = tf.squeeze(tf.stack([gen_gaps(w, v, b, seq_len) for _ in range(batch_size)]))\n",
    "        real_seq = tf.squeeze(tf.stack([tf.reshape(gen_gaps_real(seq_len), (1, seq_len))\\\n",
    "                                      for _ in range(batch_size)]))\n",
    "        d_x_true = discriminator(tf.expand_dims(tf.math.log(real_seq),-1))\n",
    "        d_x_fake = discriminator(tf.expand_dims(tf.math.log(fake_seq),-1))\n",
    "        \n",
    "        discriminator_loss = discriminator_objective(d_x_true, d_x_fake)\n",
    "        # Adjusting Gradient of Discriminator\n",
    "        gradients_of_discriminator = disc_tape.gradient(\n",
    "            discriminator_loss, discriminator.trainable_variables\n",
    "        )\n",
    "        discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "        )  # Takes a list of gradient and variables pairs\n",
    "    m.reset_states()\n",
    "    m.update_state(tf.ones_like(d_x_true),tf.math.sigmoid(d_x_true))\n",
    "    real_acc = m.result()\n",
    "\n",
    "    m.reset_states()\n",
    "    m.update_state(tf.zeros_like(d_x_fake),tf.math.sigmoid(d_x_fake))\n",
    "    fake_acc = m.result()\n",
    "\n",
    "    return discriminator_loss, real_acc, fake_acc\n",
    "  \n",
    "def training(epoches):\n",
    "    for epoch in range(epoches + 1):\n",
    "        disc_loss,real,fake = training_step(discriminator)\n",
    "        d_loss.append(disc_loss)\n",
    "        real_acc.append(real)\n",
    "        fake_acc.append(fake)\n",
    "        print(\"epoch=%d discriminator_loss=%f real_acc=%f fake_acc=%f\"\n",
    "                % (epoch, disc_loss, real, fake))\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S9jhuPJK-703"
   },
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "learning_rate = 0.0005\n",
    "batch_size = 1000\n",
    "EPOCHES = 500\n",
    "discriminator_optimizer = optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ec5wj8J8NNHq"
   },
   "outputs": [],
   "source": [
    "#upload weights.csv to colab\n",
    "w,v,b=np.loadtxt('weights.csv',delimiter=',')\n",
    "w = tf.expand_dims(w,-1)\n",
    "v = tf.expand_dims(v,-1)\n",
    "b = tf.expand_dims(b,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "NWDNT8MzOec_",
    "outputId": "6c1e1581-2078-4046-b458-927f33c89271"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAT+klEQVR4nO3df4xd5X3n8fe3xo0LJoVgrwUeh7FYx/xSsJ0JHYfVyglLMFDVVCoRVjZ4WVYmCdl1VpFSmwrRJqRytC10UTYgt4DdjdcUQSwIkK6xISKNIPQavOAfgL3OpJ5Zgwc3cX7J/PzuH3NMB3vsuTP33rmeh/dLurrnPOc553yPGD4+9znnnhuZiSSpLL/V7gIkSc1nuEtSgQx3SSqQ4S5JBTLcJalAJ7S7AIApU6ZkZ2dnu8uQpHFl8+bNr2Xm1KGWHRfh3tnZSa1Wa3cZkjSuRMRPj7bMYRlJKpDhLkkFMtwlqUDHxZi7JDXizTffpLe3l4MHD7a7lJaYNGkSHR0dTJw4se51DHdJ415vby8nn3wynZ2dRES7y2mqzGT//v309vYyc+bMutdzWEbSuHfw4EFOO+204oIdICI47bTTRvypxHCXVIQSg/2Q0Ryb4S5JBXLMXVJxOpc/0tTt9ay8Ytg+t99+O3fccQfz5s1j7dq1RyxfvXo1tVqNb33rW02t7WgMd0lND8N61ROa48W3v/1tNm7cSEdHR7tLARyWkaSGff7zn2f37t1cdtllfPOb32T+/PnMnTuXT3ziE7z00ktH9H/kkUeYP38+r732Ghs2bGD+/PnMmzePq666il/96ldNqclwl6QG3XnnnZxxxhk88cQTfOELX+CHP/whzz33HF/72te48cYb39N3/fr1rFy5kkcffRSAW265hY0bN/Lss8/S1dXFrbfe2pSaHJaRpCY6cOAAS5YsYefOnUQEb7755rvLHn/8cWq1Ghs2bOCDH/wgDz/8MNu3b+eiiy4C4I033mD+/PlNqcNwl6Qmuummm/jkJz/J+vXr6enpYcGCBe8uO+uss9i9ezcvv/wyXV1dZCaXXHIJ69ata3odDstIUhMdOHCA6dOnAwN3yAx25pln8sADD3DNNdewbds2uru7+dGPfsSuXbsA+PWvf83LL7/clDo8c5dUnHbehfPVr36VJUuWcMstt3DFFUfWcfbZZ7N27Vquuuoqvve977F69WoWL17M66+/DgyMwX/kIx9puI7IzIY30qiurq70xzqk9hnvt0Lu2LGDc845pynbOl4NdYwRsTkzu4bq77CMJBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpD3uUsqT+2e5m6v69rmbm8InZ2d1Go1pkyZ0pTtDXvmHhGTIuKZiPg/EbEtIv6sap8ZET+OiF0R8XcR8dtV+weq+V3V8s6mVCpJ40Rm8s4777S1hnqGZV4HPpWZFwBzgIUR0Q18E7gtM/818DPguqr/dcDPqvbbqn6SVLSenh5mz57NNddcw/nnn8/Xv/51Pv7xj/PRj36Um2+++d1+V155JR/72Mc477zzWLVqVcvqGTbcc8ChBwxPrF4JfAq4v2pfA1xZTS+q5qmWXxwl/7ihJFV27tzJF7/4RW677Tb6+vp45pln2LJlC5s3b+bJJ58E4O6772bz5s3UajVuv/129u/f35Ja6rqgGhETImILsA94DPi/wM8z862qSy8wvZqeDuwBqJYfAE5rZtGSdDw688wz6e7uZsOGDWzYsIG5c+cyb948XnzxRXbu3AkM/BzfBRdcQHd3N3v27Hm3vdnquqCamW8DcyLiFGA9cHajO46IpcBSgA9/+MONbk6S2u6kk04CBsbcV6xYwfXXX/+e5T/4wQ/YuHEjTz31FCeeeCILFizg4MGDLallRHfLZObPI+IJYD5wSkScUJ2ddwB9Vbc+YAbQGxEnAL8LHPG5IzNXAatg4MFhoz8ESUMawR0jiye80NCu1r19cUPrl+bSSy/lpptu4rOf/SyTJ0+mr6+PiRMncuDAAU499VROPPFEXnzxRZ5++umW1TBsuEfEVODNKth/B7iEgYukTwB/BNwLLAEerFZ5qJp/qlr+eB4Pj56U9P4xBrcuHsunP/1pduzY8e6vKk2ePJnvfOc7LFy4kDvvvJNzzjmH2bNn093d3bIa6jlzPx1YExETGBijvy8zH46I7cC9EXEL8BxwV9X/LuB/RsQu4J+Bq1tQtyQdVzo7O9m6deu788uWLWPZsmVH9Pv+978/5Po9PT1NrWfYcM/M54G5Q7TvBi4cov0gcFVTqpMkjYqPH5CkAhnukopQ8qW90Ryb4S5p3Js0aRL79+8vMuAzk/379zNp0qQRreeDwySNex0dHfT29tLf39/uUlpi0qRJdHR0jGgdw13SuDdx4kRmzpzZ7jKOKw7LSFKBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoH8JSZJDVs8YdPoVqztG1n/rmtHt5/3oWHP3CNiRkQ8ERHbI2JbRCyr2v80IvoiYkv1unzQOisiYldEvBQRl7byACRJR6rnzP0t4CuZ+WxEnAxsjojHqmW3ZeZfDO4cEecCVwPnAWcAGyPiI5n5djMLlyQd3bBn7pm5NzOfraZ/CewAph9jlUXAvZn5emb+BNgFXNiMYiVJ9RnRBdWI6ATmAj+umr4UEc9HxN0RcWrVNh3YM2i1Xob4xyAilkZELSJq/f39Iy5cknR0dYd7REwGHgC+nJm/AO4AzgLmAHuBvxzJjjNzVWZ2ZWbX1KlTR7KqJGkYdYV7RExkINjXZuZ3ATLz1cx8OzPfAf6afxl66QNmDFq9o2qTJI2Reu6WCeAuYEdm3jqo/fRB3f4Q2FpNPwRcHREfiIiZwCzgmeaVLEkaTj13y1wEfA54ISK2VG03AosjYg6QQA9wPUBmbouI+4DtDNxpc4N3ykj16Vz+SNO2tXjCC03blsafYcM9M/8BiCEWPXqMdb4BfKOBuiRJDfDxA5JUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUDDhntEzIiIJyJie0Rsi4hlVfuHIuKxiNhZvZ9atUdE3B4RuyLi+YiY1+qDkCS9Vz1n7m8BX8nMc4Fu4IaIOBdYDmzKzFnApmoe4DJgVvVaCtzR9KolScc0bLhn5t7MfLaa/iWwA5gOLALWVN3WAFdW04uAv80BTwOnRMTpTa9cknRUIxpzj4hOYC7wY2BaZu6tFr0CTKumpwN7Bq3WW7Udvq2lEVGLiFp/f/8Iy5YkHUvd4R4Rk4EHgC9n5i8GL8vMBHIkO87MVZnZlZldU6dOHcmqkqRh1BXuETGRgWBfm5nfrZpfPTTcUr3vq9r7gBmDVu+o2iRJY6Seu2UCuAvYkZm3Dlr0ELCkml4CPDio/Zrqrplu4MCg4RtJ0hg4oY4+FwGfA16IiC1V243ASuC+iLgO+CnwmWrZo8DlwC7gN8C1Ta1YkjSsYcM9M/8BiKMsvniI/gnc0GBdkqQG+A1VSSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoHq+YaqpGaq3XPURYsnvDCGhahknrlLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpCPH5DUNivWj+xxC+vuf6Qp++1ZeUVTtnM888xdkgpkuEtSgQx3SSrQsOEeEXdHxL6I2Dqo7U8joi8itlSvywctWxERuyLipYi4tFWFS5KOrp4z99XAwiHab8vMOdXrUYCIOBe4GjivWufbETGhWcVKkuoz7N0ymflkRHTWub1FwL2Z+Trwk4jYBVwIPDXqCqU26FzenLsyhuIPcmgsNDLm/qWIeL4atjm1apsO7BnUp7dqO0JELI2IWkTU+vv7GyhDknS40Yb7HcBZwBxgL/CXI91AZq7KzK7M7Jo6deooy5AkDWVU4Z6Zr2bm25n5DvDXDAy9APQBMwZ17ajaJEljaFThHhGnD5r9Q+DQnTQPAVdHxAciYiYwC3imsRIlSSM17AXViFgHLACmREQvcDOwICLmAAn0ANcDZOa2iLgP2A68BdyQmW+3pnRJ0tHUc7fM4iGa7zpG/28A32ikKElSY/yGqiQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKtCw4R4Rd0fEvojYOqjtQxHxWETsrN5PrdojIm6PiF0R8XxEzGtl8ZKkodVz5r4aWHhY23JgU2bOAjZV8wCXAbOq11LgjuaUKUkaiROG65CZT0ZE52HNi4AF1fQa4AfAH1ftf5uZCTwdEadExOmZubdZBUt6/1o8YVNzNlTbd+zlXdc2Zz9tNNox92mDAvsVYFo1PR3YM6hfb9V2hIhYGhG1iKj19/ePsgxJ0lAavqBanaXnKNZblZldmdk1derURsuQJA0y2nB/NSJOB6jeD33G6QNmDOrXUbVJksbQaMP9IWBJNb0EeHBQ+zXVXTPdwAHH2yVp7A17QTUi1jFw8XRKRPQCNwMrgfsi4jrgp8Bnqu6PApcDu4DfAOP/qoQkjUP13C2z+CiLLh6ibwI3NFqUJKkxfkNVkgpkuEtSgQx3SSrQsGPu0vtC7Z73zC6e8EKbCpGawzN3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFauhn9iKiB/gl8DbwVmZ2RcSHgL8DOoEe4DOZ+bPGytT7VefyR8ZkP/6snkrTjDP3T2bmnMzsquaXA5sycxawqZqXJI2hVgzLLALWVNNrgCtbsA9J0jE0Gu4JbIiIzRGxtGqblpl7q+lXgGlDrRgRSyOiFhG1/v7+BsuQJA3W0Jg78G8ysy8i/hXwWES8OHhhZmZE5FArZuYqYBVAV1fXkH0kSaPT0Jl7ZvZV7/uA9cCFwKsRcTpA9b6v0SIlSSMz6nCPiJMi4uRD08Cnga3AQ8CSqtsS4MFGi5QkjUwjwzLTgPURcWg7/ysz/z4i/hG4LyKuA34KfKbxMiVJIzHqcM/M3cAFQ7TvBy5upChJUmMavaAqtdTiCZvaXYI0Lvn4AUkqkOEuSQUy3CWpQI65S3rfWbH+2A+KW3d/8x9Y17PyiqZv81g8c5ekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUDe5666jNUPVR9u8YS27FYa9wx3STpMSx5YVzvK7xZ1Xdv8feGwjCQVyXCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQt0JqVPxtU+n4ZriXpHZPyza9eMKxf9xA0vHFYRlJKlDLwj0iFkbESxGxKyKWt2o/kqQjtSTcI2IC8D+Ay4BzgcURcW4r9iVJOlKrxtwvBHZl5m6AiLgXWARsb/aO2vVAq5Hw4qOksRaZ2fyNRvwRsDAz/1M1/zng9zLzS4P6LAWWVrOzgZeaXghMAV5rwXbboZRjKeU4wGM5Xr2fjuXMzJw61IK23S2TmauAVa3cR0TUMrOrlfsYK6UcSynHAR7L8cpjGdCqC6p9wIxB8x1VmyRpDLQq3P8RmBURMyPit4GrgYdatC9J0mFaMiyTmW9FxJeA/w1MAO7OzG2t2NcwWjrsM8ZKOZZSjgM8luOVx0KLLqhKktrLb6hKUoEMd0kqUJHhXsqjDyJiRkQ8ERHbI2JbRCxrd02NiogJEfFcRDzc7loaERGnRMT9EfFiROyIiPntrmm0IuK/Vn9fWyNiXURMandN9YqIuyNiX0RsHdT2oYh4LCJ2Vu+ntrPGeh3lWP5b9Tf2fESsj4hT6t1eceFe2KMP3gK+kpnnAt3ADeP4WA5ZBuxodxFN8N+Bv8/Ms4ELGKfHFBHTgf8CdGXm+QzcAHF1e6sakdXAwsPalgObMnMWsKmaHw9Wc+SxPAacn5kfBV4GVtS7seLCnUGPPsjMN4BDjz4YdzJzb2Y+W03/koEAmd7eqkYvIjqAK4C/aXctjYiI3wX+LXAXQGa+kZk/b29VDTkB+J2IOAE4Efh/ba6nbpn5JPDPhzUvAtZU02uAK8e0qFEa6lgyc0NmvlXNPs3Ad4bqUmK4Twf2DJrvZRwH4iER0QnMBX7c3koa8lfAV4F32l1Ig2YC/cA91RDT30TESe0uajQysw/4C+CfgL3Agczc0N6qGjYtM/dW068A09pZTBP9R+D79XYuMdyLExGTgQeAL2fmL9pdz2hExO8D+zJzc7traYITgHnAHZk5F/g14+ej/3tU49GLGPgH6wzgpIj49+2tqnly4F7vcX+/d0T8CQPDtGvrXafEcC/q0QcRMZGBYF+bmd9tdz0NuAj4g4joYWCo7FMR8Z32ljRqvUBvZh76FHU/A2E/Hv074CeZ2Z+ZbwLfBT7R5poa9WpEnA5Qve9rcz0NiYj/APw+8NkcwReTSgz3Yh59EBHBwLjujsy8td31NCIzV2RmR2Z2MvDf5PHMHJdniJn5CrAnImZXTRfTgsdZj5F/Aroj4sTq7+1ixunF4UEeApZU00uAB9tYS0MiYiEDQ5l/kJm/Gcm6xYV7dfHh0KMPdgD3tenRB81wEfA5Bs5yt1Svy9tdlAD4z8DaiHgemAP8eZvrGZXq08f9wLPACwxkwrj5+n5ErAOeAmZHRG9EXAesBC6JiJ0MfDJZ2c4a63WUY/kWcDLwWPX//511b8/HD0hSeYo7c5ckGe6SVCTDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQP8fAkl5z8w5ProAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = tf.math.log(gen_gaps(w, v, b, 1000))\n",
    "y = tf.math.log(gen_gaps_real(1000))\n",
    "plt.hist(x.numpy().reshape(-1),label = 'fake')\n",
    "plt.hist(y.numpy().reshape(-1), alpha=0.4,label='real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "LN8jwzyXLxoW",
    "outputId": "d0b2b7c7-d9f1-42f8-d287-842ec8fce0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 100, 16)           96        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100, 16)           64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 100, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 32)           2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 32)           128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,913\n",
      "Trainable params: 2,817\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential(\n",
    "    [\n",
    "     Input(shape=(seq_len,1)),\n",
    "     Conv1D(filters=16, kernel_size=5, padding=\"same\"),\n",
    "     BatchNormalization(),\n",
    "     LeakyReLU(),\n",
    "     Dropout(0.2),\n",
    "     Conv1D(filters=32, kernel_size=5, padding=\"same\"),\n",
    "     BatchNormalization(),\n",
    "     LeakyReLU(),\n",
    "     Dropout(0.2),\n",
    "     GlobalAveragePooling1D(),\n",
    "     Dense(1)\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3XQB77QPNFVt",
    "outputId": "ea604227-c9bc-4685-8265-ad60c7e63bcd"
   },
   "outputs": [],
   "source": [
    "d_loss = []\n",
    "real_acc = []\n",
    "fake_acc = []\n",
    "training(EPOCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "Ud8R_l_Gae02",
    "outputId": "cbfbfcf8-e5e9-4ca9-ae9a-0cda58f96f0c"
   },
   "outputs": [],
   "source": [
    "B = 20\n",
    "W = np.hamming(B)\n",
    "W /= W.sum()\n",
    "plt.figure(figsize = (21,8))\n",
    "plt.plot(range(len(real_acc)),np.convolve(real_acc, W, mode='same'),label='real')\n",
    "plt.plot(range(len(real_acc)),np.convolve(fake_acc, W, mode='same'),label='fake')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "train_D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
